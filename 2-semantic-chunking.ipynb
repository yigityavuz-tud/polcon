{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cf782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yigit\\.cache\\huggingface\\hub\\models--emrecan--bert-base-turkish-cased-mean-nli-stsb-tr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize embedding model (good for Turkish)\n",
    "model = SentenceTransformer('emrecan/bert-base-turkish-cased-mean-nli-stsb-tr')\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences\n",
    "    \"\"\"\n",
    "    # Split by sentence-ending punctuation\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def calculate_sentence_similarities(sentences: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between consecutive sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        \n",
    "    Returns:\n",
    "        List of similarity scores between consecutive sentences\n",
    "    \"\"\"\n",
    "    if len(sentences) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Create embeddings for all sentences\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Calculate cosine similarity between consecutive sentences\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        sim = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1])\n",
    "        )\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def find_split_points(similarities: List[float], threshold: float = 0.7) -> List[int]:\n",
    "    \"\"\"\n",
    "    Find indices where similarity drops below threshold (topic boundaries).\n",
    "    \n",
    "    Args:\n",
    "        similarities: List of similarity scores\n",
    "        threshold: Minimum similarity to keep sentences together\n",
    "        \n",
    "    Returns:\n",
    "        List of sentence indices where splits should occur\n",
    "    \"\"\"\n",
    "    split_points = []\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < threshold:\n",
    "            split_points.append(i + 1)  # Split after sentence i\n",
    "    \n",
    "    return split_points\n",
    "\n",
    "\n",
    "def create_chunks_with_overlap(\n",
    "    sentences: List[str],\n",
    "    split_points: List[int],\n",
    "    max_chunk_size: int = 1000,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Create chunks at split points with overlap.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        split_points: Indices where splits should occur\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        overlap_percent: Percentage of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    split_points = [0] + split_points + [len(sentences)]\n",
    "    \n",
    "    for i in range(len(split_points) - 1):\n",
    "        start_idx = split_points[i]\n",
    "        end_idx = split_points[i + 1]\n",
    "        \n",
    "        # Get sentences for this chunk\n",
    "        chunk_sentences = sentences[start_idx:end_idx]\n",
    "        chunk_text = ' '.join(chunk_sentences)\n",
    "        \n",
    "        # If chunk is too large, split it further by max_chunk_size\n",
    "        if len(chunk_text) > max_chunk_size:\n",
    "            sub_chunks = split_large_chunk(chunk_sentences, max_chunk_size)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk_text)\n",
    "    \n",
    "    # Add overlap between chunks\n",
    "    chunks_with_overlap = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i == 0:\n",
    "            chunks_with_overlap.append(chunk)\n",
    "        else:\n",
    "            # Calculate overlap size\n",
    "            overlap_size = int(len(chunks[i - 1]) * overlap_percent)\n",
    "            overlap_text = chunks[i - 1][-overlap_size:] if overlap_size > 0 else \"\"\n",
    "            \n",
    "            # Add overlap from previous chunk\n",
    "            chunks_with_overlap.append(overlap_text + \" \" + chunk)\n",
    "    \n",
    "    return chunks_with_overlap\n",
    "\n",
    "\n",
    "def split_large_chunk(sentences: List[str], max_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a large chunk into smaller chunks by size.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        max_size: Maximum chunk size\n",
    "        \n",
    "    Returns:\n",
    "        List of chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "        \n",
    "        if current_size + sentence_size > max_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def semantic_chunk(\n",
    "    text: str,\n",
    "    max_chunk_size: int = 1000,\n",
    "    similarity_threshold: float = 0.7,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Main semantic chunking function.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        similarity_threshold: Similarity threshold for splitting (lower = more splits)\n",
    "        overlap_percent: Percentage of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    \n",
    "    if len(sentences) == 1:\n",
    "        return [sentences[0]]\n",
    "    \n",
    "    # Calculate semantic similarities\n",
    "    print(\"  Calculating sentence similarities...\", end=\" \")\n",
    "    similarities = calculate_sentence_similarities(sentences)\n",
    "    print(\"✓\")\n",
    "    \n",
    "    # Find split points based on similarity drops\n",
    "    split_points = find_split_points(similarities, similarity_threshold)\n",
    "    \n",
    "    # Create chunks with overlap\n",
    "    chunks = create_chunks_with_overlap(\n",
    "        sentences, split_points, max_chunk_size, overlap_percent\n",
    "    )\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_json_file(\n",
    "    json_path: Path,\n",
    "    max_chunk_size: int = 1000,\n",
    "    similarity_threshold: float = 0.7,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single JSON file and create semantic chunks.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        similarity_threshold: Similarity threshold for splitting\n",
    "        overlap_percent: Percentage of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = data.get('text', '')\n",
    "    metadata = data.get('metadata', {})\n",
    "    \n",
    "    # Create semantic chunks\n",
    "    chunks = semantic_chunk(text, max_chunk_size, similarity_threshold, overlap_percent)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    result = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        result.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk_text,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"source_file\": metadata.get('source_file', ''),\n",
    "            \"source_path\": metadata.get('source_path', '')\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def save_chunks_jsonl(chunks: List[Dict], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Save chunks to JSONL file (one JSON object per line).\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        output_path: Path for output JSONL file\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            json.dump(chunk, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def process_all_json_files(\n",
    "    input_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\text\",\n",
    "    output_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\chunks\",\n",
    "    max_chunk_size: int = 1000,\n",
    "    similarity_threshold: float = 0.7,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process all JSON files and create semantic chunks with overlap.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing JSON files\n",
    "        output_dir: Directory to save chunked JSONL files\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        similarity_threshold: Lower = more splits (0.6-0.8 recommended)\n",
    "        overlap_percent: Overlap percentage (0.1 = 10%)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = list(input_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    print(f\"Settings: max_chunk_size={max_chunk_size}, threshold={similarity_threshold}, overlap={overlap_percent*100}%\\n\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            print(f\"Processing: {json_file.name}\")\n",
    "            \n",
    "            # Create semantic chunks\n",
    "            chunks = process_json_file(\n",
    "                json_file, max_chunk_size, similarity_threshold, overlap_percent\n",
    "            )\n",
    "            \n",
    "            # Save as JSONL\n",
    "            output_file = output_path / f\"{json_file.stem}_chunks.jsonl\"\n",
    "            save_chunks_jsonl(chunks, output_file)\n",
    "            \n",
    "            total_chunks += len(chunks)\n",
    "            print(f\"  ✓ Created {len(chunks)} chunks → {output_file.name}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\\n\")\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Total chunks created: {total_chunks}\")\n",
    "    print(f\"Files saved to: {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525b16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 JSON files to process\n",
      "Settings: max_chunk_size=1000, threshold=0.7, overlap=10.0%\n",
      "\n",
      "Processing: 1) Temel kavramlar önyargı, kalıpyargı ve ayrımcılık.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 183 chunks → 1) Temel kavramlar önyargı, kalıpyargı ve ayrımcılık_chunks.jsonl\n",
      "\n",
      "Processing: 10) TÜRKİYE’DE ÖRGÜTLENME ÖZGÜRLÜĞÜNÜN GENEL GÖRÜNÜMÜ-II .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1233 chunks → 10) TÜRKİYE’DE ÖRGÜTLENME ÖZGÜRLÜĞÜNÜN GENEL GÖRÜNÜMÜ-II _chunks.jsonl\n",
      "\n",
      "Processing: 11) Yurttaslik_Alani_Bilgi_Notu_1.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 195 chunks → 11) Yurttaslik_Alani_Bilgi_Notu_1_chunks.jsonl\n",
      "\n",
      "Processing: 12) TERÖRLE MÜCADELEYİ ARAÇSALLAŞTIRMAK.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 370 chunks → 12) TERÖRLE MÜCADELEYİ ARAÇSALLAŞTIRMAK_chunks.jsonl\n",
      "\n",
      "Processing: 13) PROTESTO HAKKINI KORU.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 557 chunks → 13) PROTESTO HAKKINI KORU_chunks.jsonl\n",
      "\n",
      "Processing: 14) KomploTeorileri_AR_23.03.23_web.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 685 chunks → 14) KomploTeorileri_AR_23.03.23_web_chunks.jsonl\n",
      "\n",
      "Processing: 15) Feminist_Hareketin_Gundemleri_.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 260 chunks → 15) Feminist_Hareketin_Gundemleri__chunks.jsonl\n",
      "\n",
      "Processing: 16) Sivil Toplum Kuruluşlarının Devlet Tarafından Finansmanı Üzerine Bir Tartışma.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 281 chunks → 16) Sivil Toplum Kuruluşlarının Devlet Tarafından Finansmanı Üzerine Bir Tartışma_chunks.jsonl\n",
      "\n",
      "Processing: 17) Gençlik Politikalarında Karşılaştırmalı Bir Değerlendirme-Türkiye ve Finlandiya Örneği.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 518 chunks → 17) Gençlik Politikalarında Karşılaştırmalı Bir Değerlendirme-Türkiye ve Finlandiya Örneği_chunks.jsonl\n",
      "\n",
      "Processing: 18) Avrupa Konseyi Politik Karar Alma Süreçlerine Sivil Katılım Rehberi Çevirisi.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 175 chunks → 18) Avrupa Konseyi Politik Karar Alma Süreçlerine Sivil Katılım Rehberi Çevirisi_chunks.jsonl\n",
      "\n",
      "Processing: 19) Kampüsten Öğrenci Toplulukları .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 2210 chunks → 19) Kampüsten Öğrenci Toplulukları _chunks.jsonl\n",
      "\n",
      "Processing: 2) Ayrımcılık ve medya.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 212 chunks → 2) Ayrımcılık ve medya_chunks.jsonl\n",
      "\n",
      "Processing: 20) Gençler Ne(ler) İstiyor_ .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 222 chunks → 20) Gençler Ne(ler) İstiyor_ _chunks.jsonl\n",
      "\n",
      "Processing: 21) Türkiye’de Gençlik ve Siyaset_ Gelecek İçin Nasıl Bir Katılım_ .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 642 chunks → 21) Türkiye’de Gençlik ve Siyaset_ Gelecek İçin Nasıl Bir Katılım_ _chunks.jsonl\n",
      "\n",
      "Processing: 22) Gençlik Araştırmaları Dergisi 13.sayı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 3866 chunks → 22) Gençlik Araştırmaları Dergisi 13.sayı_chunks.jsonl\n",
      "\n",
      "Processing: 23) Türkiye_de Gençlik Miti 1980 Sonrası Türkiye Gençliği İletişim Yayınları.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 3825 chunks → 23) Türkiye_de Gençlik Miti 1980 Sonrası Türkiye Gençliği İletişim Yayınları_chunks.jsonl\n",
      "\n",
      "Processing: 24) Türkiye’nin Gençliği Araştırması Raporu -SODEV- .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 69 chunks → 24) Türkiye’nin Gençliği Araştırması Raporu -SODEV- _chunks.jsonl\n",
      "\n",
      "Processing: 25) Türkiye’de Gençlerin Güvencesizliği_ Çalışma, Geçim ve Yaşam Algısı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1064 chunks → 25) Türkiye’de Gençlerin Güvencesizliği_ Çalışma, Geçim ve Yaşam Algısı_chunks.jsonl\n",
      "\n",
      "Processing: 26) Toplumun Boğaziçi Üniversitesi Olaylarına Bakışı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 251 chunks → 26) Toplumun Boğaziçi Üniversitesi Olaylarına Bakışı_chunks.jsonl\n",
      "\n",
      "Processing: 27) Kürt Gençler’20 Benzerlikler Farklar Değişimler.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 785 chunks → 27) Kürt Gençler’20 Benzerlikler Farklar Değişimler_chunks.jsonl\n",
      "\n",
      "Processing: 28) NEET Gençler Araştırması – NEET Gençlerin İnsan Onuruna Yaraşır Yaşam Sürme Hakkına Erişimi.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1162 chunks → 28) NEET Gençler Araştırması – NEET Gençlerin İnsan Onuruna Yaraşır Yaşam Sürme Hakkına Erişimi_chunks.jsonl\n",
      "\n",
      "Processing: 29) TGSP Türkiye’nin Gençleri Araştırması.pdf.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 396 chunks → 29) TGSP Türkiye’nin Gençleri Araştırması.pdf_chunks.jsonl\n",
      "\n",
      "Processing: 3) Toplumsal Cinsiyete Dayalı Ayrımcılık.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 201 chunks → 3) Toplumsal Cinsiyete Dayalı Ayrımcılık_chunks.jsonl\n",
      "\n",
      "Processing: 30) TOG Gençlik Çalışmasının Toplumsal Katılıma Etkisi Araştırması.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1317 chunks → 30) TOG Gençlik Çalışmasının Toplumsal Katılıma Etkisi Araştırması_chunks.jsonl\n",
      "\n",
      "Processing: 31) Türkiye’de Gençlerin İyi Olma Hali Saha Araştırması Bulguları- HABİTAT- .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1689 chunks → 31) Türkiye’de Gençlerin İyi Olma Hali Saha Araştırması Bulguları- HABİTAT- _chunks.jsonl\n",
      "\n",
      "Processing: 32) Türkiye Gençlik Araştırması 2021.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 8093 chunks → 32) Türkiye Gençlik Araştırması 2021_chunks.jsonl\n",
      "\n",
      "Processing: 33) Türkiye’nin Gençliği Araştırması Raporu -SODEV- 2020.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 66 chunks → 33) Türkiye’nin Gençliği Araştırması Raporu -SODEV- 2020_chunks.jsonl\n",
      "\n",
      "Processing: 34)Uluslararası Af Örgütü.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1387 chunks → 34)Uluslararası Af Örgütü_chunks.jsonl\n",
      "\n",
      "Processing: 37) Perspectives on Youth Participation.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 381 chunks → 37) Perspectives on Youth Participation_chunks.jsonl\n",
      "\n",
      "Processing: 38) Young people’s right to assemble peacefully.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 526 chunks → 38) Young people’s right to assemble peacefully_chunks.jsonl\n",
      "\n",
      "Processing: 39) Shrinking democratic civic space for youth.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 487 chunks → 39) Shrinking democratic civic space for youth_chunks.jsonl\n",
      "\n",
      "Processing: 4) Uluslararası Af Örgütü Raporu 2021-2022 Avrupa ve Orta Asya Değerlendirmesi(sayfa 46-54).json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 220 chunks → 4) Uluslararası Af Örgütü Raporu 2021-2022 Avrupa ve Orta Asya Değerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "\n",
      "Processing: 40) Türkiye’de Genç İntiharları.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1343 chunks → 40) Türkiye’de Genç İntiharları_chunks.jsonl\n",
      "\n",
      "Processing: 41) Türkiye’de Genç İntiharları Politika Önerileri.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 43 chunks → 41) Türkiye’de Genç İntiharları Politika Önerileri_chunks.jsonl\n",
      "\n",
      "Processing: 42) Türkiye_de ifade ve medya özgürlüğü ve insan hakları savunucuları ile sivil toplumun durumu hakkındaki memorandum.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 368 chunks → 42) Türkiye_de ifade ve medya özgürlüğü ve insan hakları savunucuları ile sivil toplumun durumu hakkındaki memorandum_chunks.jsonl\n",
      "\n",
      "Processing: 43) Türkiye’deki Gençlik Örgütlerinin İhtiyaç Analizi Raporu 2025.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1033 chunks → 43) Türkiye’deki Gençlik Örgütlerinin İhtiyaç Analizi Raporu 2025_chunks.jsonl\n",
      "\n",
      "Processing: 44)Toplumsal Değerler ve Gençlik Araştırma Raporu.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 363 chunks → 44)Toplumsal Değerler ve Gençlik Araştırma Raporu_chunks.jsonl\n",
      "\n",
      "Processing: 45) Gençlerin Politik Tercihleri Araştırması.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 922 chunks → 45) Gençlerin Politik Tercihleri Araştırması_chunks.jsonl\n",
      "\n",
      "Processing: 46)Gençlik Araştırmaları Dergisi 35.sayı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 3582 chunks → 46)Gençlik Araştırmaları Dergisi 35.sayı_chunks.jsonl\n",
      "\n",
      "Processing: 47)Gençler için nasıl bir kent_.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 198 chunks → 47)Gençler için nasıl bir kent__chunks.jsonl\n",
      "\n",
      "Processing: 48) TGSP Gençlerin Gönüllülük Algısı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 184 chunks → 48) TGSP Gençlerin Gönüllülük Algısı_chunks.jsonl\n",
      "\n",
      "Processing: 49) Yerel Yonetimlere Iliskin PolitikaBelgesi.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 94 chunks → 49) Yerel Yonetimlere Iliskin PolitikaBelgesi_chunks.jsonl\n",
      "\n",
      "Processing: 5) Paralel Kariyer Arayışının Nedenleri Isparta’da Faaliyet Gösteren STK’larda Bir Araştırma.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 249 chunks → 5) Paralel Kariyer Arayışının Nedenleri Isparta’da Faaliyet Gösteren STK’larda Bir Araştırma_chunks.jsonl\n",
      "\n",
      "Processing: 50) Milliyetçiliğin Dönüşümü ve Genç Yüzleri.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 2300 chunks → 50) Milliyetçiliğin Dönüşümü ve Genç Yüzleri_chunks.jsonl\n",
      "\n",
      "Processing: 51)Gençlik Araştırması - Türkiye 2024.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 606 chunks → 51)Gençlik Araştırması - Türkiye 2024_chunks.jsonl\n",
      "\n",
      "Processing: 52)CORE Gençlerin Seçimi.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 257 chunks → 52)CORE Gençlerin Seçimi_chunks.jsonl\n",
      "\n",
      "Processing: 53) Gençlerin Güçlendirilmesine Yönelik Harcamaları İzleme Kılavuzu.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 4 chunks → 53) Gençlerin Güçlendirilmesine Yönelik Harcamaları İzleme Kılavuzu_chunks.jsonl\n",
      "\n",
      "Processing: 54) Gençlerin Siyasi Katılımı - Şebeke 1.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 3049 chunks → 54) Gençlerin Siyasi Katılımı - Şebeke 1_chunks.jsonl\n",
      "\n",
      "Processing: 55) Türkiye_de Gençlerin Katılımı - Şebeke.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1083 chunks → 55) Türkiye_de Gençlerin Katılımı - Şebeke_chunks.jsonl\n",
      "\n",
      "Processing: 56) COVID-19 Pandemisi Sürecinde Gençlerin İyilik Halinin Belirlenmesi Araştırması.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1930 chunks → 56) COVID-19 Pandemisi Sürecinde Gençlerin İyilik Halinin Belirlenmesi Araştırması_chunks.jsonl\n",
      "\n",
      "Processing: 57) KONDA - Hafıza Merkezi Gençlerin İnsan Hakları Algısı_.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1010 chunks → 57) KONDA - Hafıza Merkezi Gençlerin İnsan Hakları Algısı__chunks.jsonl\n",
      "\n",
      "Processing: 58) Türkiye_de Gençlik Çalışması ve Politikası.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 5330 chunks → 58) Türkiye_de Gençlik Çalışması ve Politikası_chunks.jsonl\n",
      "\n",
      "Processing: 59) LGBTİ gençler gençlik merkezlerinde ne istiyor_.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 215 chunks → 59) LGBTİ gençler gençlik merkezlerinde ne istiyor__chunks.jsonl\n",
      "\n",
      "Processing: 6) Sivil Toplum Örgütlerinde Profesyonel ve Gönüllü Çalışma İlişkileri Tehditler Ve Fırsatlar.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 254 chunks → 6) Sivil Toplum Örgütlerinde Profesyonel ve Gönüllü Çalışma İlişkileri Tehditler Ve Fırsatlar_chunks.jsonl\n",
      "\n",
      "Processing: 60) Genç Kadınların Karar Alma Mekanizmalarına Katılımı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 162 chunks → 60) Genç Kadınların Karar Alma Mekanizmalarına Katılımı_chunks.jsonl\n",
      "\n",
      "Processing: 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1138 chunks → 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi_chunks.jsonl\n",
      "\n",
      "Processing: 62) TGSP Türkiye_nin Gençleri Yurtdışı Algısı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 55 chunks → 62) TGSP Türkiye_nin Gençleri Yurtdışı Algısı_chunks.jsonl\n",
      "\n",
      "Processing: 63) TGSP Türkiye_nin Gençleri Dindarlık Algısı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 196 chunks → 63) TGSP Türkiye_nin Gençleri Dindarlık Algısı_chunks.jsonl\n",
      "\n",
      "Processing: 64) TGSP Türkiye_nin Gençleri Yükseköğrenim Algısı.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 265 chunks → 64) TGSP Türkiye_nin Gençleri Yükseköğrenim Algısı_chunks.jsonl\n",
      "\n",
      "Processing: 65 ) Türkiye Gençlik Araştırması 2023.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 244 chunks → 65 ) Türkiye Gençlik Araştırması 2023_chunks.jsonl\n",
      "\n",
      "Processing: 66) SODEV Genclik Arastirmasi Raporu 2021.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 61 chunks → 66) SODEV Genclik Arastirmasi Raporu 2021_chunks.jsonl\n",
      "\n",
      "Processing: 67) SERHAT TRA2 Gençlik Araştırması.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 5485 chunks → 67) SERHAT TRA2 Gençlik Araştırması_chunks.jsonl\n",
      "\n",
      "Processing: 68) İPA İstanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1288 chunks → 68) İPA İstanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim_chunks.jsonl\n",
      "\n",
      "Processing: 69) İPA Üniversite Mezunu Ev Gençleri Araştırması.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 55 chunks → 69) İPA Üniversite Mezunu Ev Gençleri Araştırması_chunks.jsonl\n",
      "\n",
      "Processing: 7) eşitsiz demokrasiler.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 348 chunks → 7) eşitsiz demokrasiler_chunks.jsonl\n",
      "\n",
      "Processing: 70) IPM Türkiye_de Gençlerin Yurtdışında Yaşama İsteği.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 104 chunks → 70) IPM Türkiye_de Gençlerin Yurtdışında Yaşama İsteği_chunks.jsonl\n",
      "\n",
      "Processing: 71) İPM Türkiye_de Akıllı Kentleşme ve Gençlik Politikaları.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 110 chunks → 71) İPM Türkiye_de Akıllı Kentleşme ve Gençlik Politikaları_chunks.jsonl\n",
      "\n",
      "Processing: 72) TÜSES Gençlerin Cinsel Sağlık ve Üreme Sağlığı Araştırması.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1452 chunks → 72) TÜSES Gençlerin Cinsel Sağlık ve Üreme Sağlığı Araştırması_chunks.jsonl\n",
      "\n",
      "Processing: 73) FES Youth Study Southeast Europe .json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1920 chunks → 73) FES Youth Study Southeast Europe _chunks.jsonl\n",
      "\n",
      "Processing: 74) Biarada İstanbul’da Gençlik, Kent Yurttaşlığı ve Yerel Yönetim.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1647 chunks → 74) Biarada İstanbul’da Gençlik, Kent Yurttaşlığı ve Yerel Yönetim_chunks.jsonl\n",
      "\n",
      "Processing: 75) Türkiye Gençlik Araştırması Özet Bulgular 2023.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 186 chunks → 75) Türkiye Gençlik Araştırması Özet Bulgular 2023_chunks.jsonl\n",
      "\n",
      "Processing: 76) Toplum Çalışması Enstitüsü Kim Bu Gençler_.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 101 chunks → 76) Toplum Çalışması Enstitüsü Kim Bu Gençler__chunks.jsonl\n",
      "\n",
      "Processing: 77) KAOS GL LGBTİ+ Öğrenciler.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1210 chunks → 77) KAOS GL LGBTİ+ Öğrenciler_chunks.jsonl\n",
      "\n",
      "Processing: 78) TOG Üniversiteli Gençlerin İhtiyaçları Araştırması 2024.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 288 chunks → 78) TOG Üniversiteli Gençlerin İhtiyaçları Araştırması 2024_chunks.jsonl\n",
      "\n",
      "Processing: 79) Haberlerdeki Üniversite 2022.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 538 chunks → 79) Haberlerdeki Üniversite 2022_chunks.jsonl\n",
      "\n",
      "Processing: 8) genc-oy-strateji-rapor.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1264 chunks → 8) genc-oy-strateji-rapor_chunks.jsonl\n",
      "\n",
      "Processing: 80) TOG Gençlerin İhtiyaçları Araştırması 2022.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 128 chunks → 80) TOG Gençlerin İhtiyaçları Araştırması 2022_chunks.jsonl\n",
      "\n",
      "Processing: 81) Yereliz GENÇLİK ALANINDA ÇALIŞAN SİVİL TOPLUM ÖRGÜTLERİ İÇİN YEREL SAVUNUCULUK REHBERİ.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 345 chunks → 81) Yereliz GENÇLİK ALANINDA ÇALIŞAN SİVİL TOPLUM ÖRGÜTLERİ İÇİN YEREL SAVUNUCULUK REHBERİ_chunks.jsonl\n",
      "\n",
      "Processing: 82) KONDA Barometre 2024.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 258 chunks → 82) KONDA Barometre 2024_chunks.jsonl\n",
      "\n",
      "Processing: 83) OECD Youth Policy Toolkit.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 3058 chunks → 83) OECD Youth Policy Toolkit_chunks.jsonl\n",
      "\n",
      "Processing: 84) TİP_li Öğrenciler Barınma Raporu 2023.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 706 chunks → 84) TİP_li Öğrenciler Barınma Raporu 2023_chunks.jsonl\n",
      "\n",
      "Processing: 85) ILO Global Employement Trends for Youth 2020.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 2338 chunks → 85) ILO Global Employement Trends for Youth 2020_chunks.jsonl\n",
      "\n",
      "Processing: 86) GoFor Hangi Genç_.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 505 chunks → 86) GoFor Hangi Genç__chunks.jsonl\n",
      "\n",
      "Processing: 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 63 chunks → 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu_chunks.jsonl\n",
      "\n",
      "Processing: 88) Türkiye_de NEET Üzerine Yapılmış Çalışmalara İlişkin Bir Değerlendirme 2024.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 501 chunks → 88) Türkiye_de NEET Üzerine Yapılmış Çalışmalara İlişkin Bir Değerlendirme 2024_chunks.jsonl\n",
      "\n",
      "Processing: 89) UNFPA İstatisliklerle Gençlik.json\n",
      "  ✓ Created 1 chunks → 89) UNFPA İstatisliklerle Gençlik_chunks.jsonl\n",
      "\n",
      "Processing: 9) Politik Karar Verme Süreçlerine Etkili ve Anlamlı KATILIM HAKKI ve MEKANİZMALAR.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 1786 chunks → 9) Politik Karar Verme Süreçlerine Etkili ve Anlamlı KATILIM HAKKI ve MEKANİZMALAR_chunks.jsonl\n",
      "\n",
      "Processing: 90) FES Genclerin Gözünden Dindar-Seküler Eksenli Kutuplaşma.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 3230 chunks → 90) FES Genclerin Gözünden Dindar-Seküler Eksenli Kutuplaşma_chunks.jsonl\n",
      "\n",
      "Processing: 91) Veriler.json\n",
      "  Calculating sentence similarities... ✓\n",
      "  ✓ Created 92 chunks → 91) Veriler_chunks.jsonl\n",
      "\n",
      "Processing complete!\n",
      "Total chunks created: 89073\n",
      "Files saved to: C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\chunks\n"
     ]
    }
   ],
   "source": [
    "# Run the chunker\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSON files with semantic chunking and 10% overlap\n",
    "    process_all_json_files(\n",
    "        max_chunk_size=1000,\n",
    "        similarity_threshold=0.7,  # Adjust between 0.6-0.8\n",
    "        overlap_percent=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43978d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de154d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\chunks\n",
      "Total files: 89 | Total chunks: 89073\n",
      "\n",
      "Global character counts:\n",
      "  count=89073 min=2 max=926151 mean=185.1 median=122.0 p95=430.0 std=4007.4\n",
      "Global word counts:\n",
      "  count=89073 min=1 max=4202 mean=22.4 median=16.0 p95=57.0 std=39.5\n",
      "\n",
      "Top-5 files by mean char_count:\n",
      "  53) Gençlerin Güçlendirilmesine Yönelik Harcamaları İzleme Kılavuzu_chunks.jsonl mean=500441.0 (chunks=4)\n",
      "  89) UNFPA İstatisliklerle Gençlik_chunks.jsonl             mean=3269.0 (chunks=1)\n",
      "  64) TGSP Türkiye_nin Gençleri Yükseköğrenim Algısı_chunks.jsonl mean=547.0 (chunks=265)\n",
      "  63) TGSP Türkiye_nin Gençleri Dindarlık Algısı_chunks.jsonl mean=522.0 (chunks=196)\n",
      "  48) TGSP Gençlerin Gönüllülük Algısı_chunks.jsonl            mean=468.1 (chunks=184)\n"
     ]
    }
   ],
   "source": [
    "# Utilities to compute chunk size statistics over a folder of JSONL chunk files\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import math\n",
    "\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read a JSONL file and return a list of dicts.\"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed lines but continue\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "\n",
    "def summarize(values: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Return summary stats for a numeric list.\"\"\"\n",
    "    if not values:\n",
    "        return {\n",
    "            'count': 0,\n",
    "            'min': 0,\n",
    "            'max': 0,\n",
    "            'mean': 0,\n",
    "            'median': 0,\n",
    "            'p95': 0,\n",
    "            'std': 0,\n",
    "        }\n",
    "    n = len(values)\n",
    "    values_sorted = sorted(values)\n",
    "    total = sum(values)\n",
    "    mean = total / n\n",
    "    # Median\n",
    "    if n % 2 == 1:\n",
    "        median = values_sorted[n // 2]\n",
    "    else:\n",
    "        median = (values_sorted[n // 2 - 1] + values_sorted[n // 2]) / 2\n",
    "    # p95\n",
    "    p95_index = min(n - 1, max(0, int(math.ceil(0.95 * n) - 1)))\n",
    "    p95 = values_sorted[p95_index]\n",
    "    # std (population std)\n",
    "    var = sum((x - mean) ** 2 for x in values) / n\n",
    "    std = math.sqrt(var)\n",
    "    return {\n",
    "        'count': n,\n",
    "        'min': values_sorted[0],\n",
    "        'max': values_sorted[-1],\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'p95': p95,\n",
    "        'std': std,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_chunks_folder(folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Compute per-file and global stats for chunk JSONL files in a folder.\"\"\"\n",
    "    folder = folder.resolve()\n",
    "    files = sorted(folder.glob('*.jsonl'))\n",
    "    per_file: Dict[str, Any] = {}\n",
    "    all_chars: List[int] = []\n",
    "    all_words: List[int] = []\n",
    "\n",
    "    for fp in files:\n",
    "        rows = read_jsonl(fp)\n",
    "        char_counts = []\n",
    "        word_counts = []\n",
    "        for r in rows:\n",
    "            # prefer explicit fields; fallback to computing from text if missing\n",
    "            if 'char_count' in r and isinstance(r['char_count'], (int, float)):\n",
    "                char_counts.append(int(r['char_count']))\n",
    "            elif 'text' in r:\n",
    "                char_counts.append(len(r['text']))\n",
    "            if 'word_count' in r and isinstance(r['word_count'], (int, float)):\n",
    "                word_counts.append(int(r['word_count']))\n",
    "            elif 'text' in r:\n",
    "                word_counts.append(len(r['text'].split()))\n",
    "        per_file[fp.name] = {\n",
    "            'files_counted': len(rows),\n",
    "            'char_stats': summarize(char_counts),\n",
    "            'word_stats': summarize(word_counts),\n",
    "        }\n",
    "        all_chars.extend(char_counts)\n",
    "        all_words.extend(word_counts)\n",
    "\n",
    "    global_stats = {\n",
    "        'total_files': len(files),\n",
    "        'total_chunks': len(all_chars),\n",
    "        'char_stats': summarize(all_chars),\n",
    "        'word_stats': summarize(all_words),\n",
    "    }\n",
    "    return {\n",
    "        'folder': str(folder),\n",
    "        'global': global_stats,\n",
    "        'per_file': per_file,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_stats_report(report: Dict[str, Any]) -> None:\n",
    "    \"\"\"Pretty-print a compact stats report.\"\"\"\n",
    "    g = report['global']\n",
    "    print(f\"Folder: {report['folder']}\")\n",
    "    print(f\"Total files: {g['total_files']} | Total chunks: {g['total_chunks']}\")\n",
    "    cs = g['char_stats']\n",
    "    ws = g['word_stats']\n",
    "    print(\"\\nGlobal character counts:\")\n",
    "    print(f\"  count={cs['count']} min={cs['min']} max={cs['max']} mean={cs['mean']:.1f} median={cs['median']:.1f} p95={cs['p95']:.1f} std={cs['std']:.1f}\")\n",
    "    print(\"Global word counts:\")\n",
    "    print(f\"  count={ws['count']} min={ws['min']} max={ws['max']} mean={ws['mean']:.1f} median={ws['median']:.1f} p95={ws['p95']:.1f} std={ws['std']:.1f}\")\n",
    "\n",
    "    # Show top-5 largest files by mean char count\n",
    "    print(\"\\nTop-5 files by mean char_count:\")\n",
    "    rows = []\n",
    "    for fn, stats in report['per_file'].items():\n",
    "        rows.append((fn, stats['char_stats']['mean'], stats['files_counted']))\n",
    "    for fn, mean_val, cnt in sorted(rows, key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  {fn:60s} mean={mean_val:.1f} (chunks={cnt})\")\n",
    "\n",
    "\n",
    "def save_stats(report: Dict[str, Any], out_json: Path = None) -> None:\n",
    "    if out_json is None:\n",
    "        return\n",
    "    with out_json.open('w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# Example: analyze the default 'chunks' folder in the repo root\n",
    "# You can change this path if needed.\n",
    "if __name__ == '__main__':\n",
    "    chunks_dir = Path(r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\chunks\")\n",
    "    report = analyze_chunks_folder(chunks_dir)\n",
    "    print_stats_report(report)\n",
    "    # Optionally save to JSON next to the folder\n",
    "    # save_stats(report, chunks_dir / 'chunks_stats.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d4572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
