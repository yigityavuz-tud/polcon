{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cf782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yigit\\.cache\\huggingface\\hub\\models--emrecan--bert-base-turkish-cased-mean-nli-stsb-tr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize embedding model (good for Turkish)\n",
    "model = SentenceTransformer('emrecan/bert-base-turkish-cased-mean-nli-stsb-tr')\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences\n",
    "    \"\"\"\n",
    "    # Split by sentence-ending punctuation\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def calculate_sentence_similarities(sentences: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between consecutive sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        \n",
    "    Returns:\n",
    "        List of similarity scores between consecutive sentences\n",
    "    \"\"\"\n",
    "    if len(sentences) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Create embeddings for all sentences\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Calculate cosine similarity between consecutive sentences\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        sim = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1])\n",
    "        )\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def find_split_points(similarities: List[float], threshold: float = 0.7) -> List[int]:\n",
    "    \"\"\"\n",
    "    Find indices where similarity drops below threshold (topic boundaries).\n",
    "    \n",
    "    Args:\n",
    "        similarities: List of similarity scores\n",
    "        threshold: Minimum similarity to keep sentences together\n",
    "        \n",
    "    Returns:\n",
    "        List of sentence indices where splits should occur\n",
    "    \"\"\"\n",
    "    split_points = []\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < threshold:\n",
    "            split_points.append(i + 1)  # Split after sentence i\n",
    "    \n",
    "    return split_points\n",
    "\n",
    "\n",
    "def create_chunks_with_overlap(\n",
    "    sentences: List[str],\n",
    "    split_points: List[int],\n",
    "    max_chunk_size: int = 1000,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Create chunks at split points with overlap.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        split_points: Indices where splits should occur\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        overlap_percent: Percentage of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    split_points = [0] + split_points + [len(sentences)]\n",
    "    \n",
    "    for i in range(len(split_points) - 1):\n",
    "        start_idx = split_points[i]\n",
    "        end_idx = split_points[i + 1]\n",
    "        \n",
    "        # Get sentences for this chunk\n",
    "        chunk_sentences = sentences[start_idx:end_idx]\n",
    "        chunk_text = ' '.join(chunk_sentences)\n",
    "        \n",
    "        # If chunk is too large, split it further by max_chunk_size\n",
    "        if len(chunk_text) > max_chunk_size:\n",
    "            sub_chunks = split_large_chunk(chunk_sentences, max_chunk_size)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk_text)\n",
    "    \n",
    "    # Add overlap between chunks\n",
    "    chunks_with_overlap = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i == 0:\n",
    "            chunks_with_overlap.append(chunk)\n",
    "        else:\n",
    "            # Calculate overlap size\n",
    "            overlap_size = int(len(chunks[i - 1]) * overlap_percent)\n",
    "            overlap_text = chunks[i - 1][-overlap_size:] if overlap_size > 0 else \"\"\n",
    "            \n",
    "            # Add overlap from previous chunk\n",
    "            chunks_with_overlap.append(overlap_text + \" \" + chunk)\n",
    "    \n",
    "    return chunks_with_overlap\n",
    "\n",
    "\n",
    "def split_large_chunk(sentences: List[str], max_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a large chunk into smaller chunks by size.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        max_size: Maximum chunk size\n",
    "        \n",
    "    Returns:\n",
    "        List of chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "        \n",
    "        if current_size + sentence_size > max_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def semantic_chunk(\n",
    "    text: str,\n",
    "    max_chunk_size: int = 1000,\n",
    "    similarity_threshold: float = 0.7,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Main semantic chunking function.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        similarity_threshold: Similarity threshold for splitting (lower = more splits)\n",
    "        overlap_percent: Percentage of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    \n",
    "    if len(sentences) == 1:\n",
    "        return [sentences[0]]\n",
    "    \n",
    "    # Calculate semantic similarities\n",
    "    print(\"  Calculating sentence similarities...\", end=\" \")\n",
    "    similarities = calculate_sentence_similarities(sentences)\n",
    "    print(\"âœ“\")\n",
    "    \n",
    "    # Find split points based on similarity drops\n",
    "    split_points = find_split_points(similarities, similarity_threshold)\n",
    "    \n",
    "    # Create chunks with overlap\n",
    "    chunks = create_chunks_with_overlap(\n",
    "        sentences, split_points, max_chunk_size, overlap_percent\n",
    "    )\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_json_file(\n",
    "    json_path: Path,\n",
    "    max_chunk_size: int = 1000,\n",
    "    similarity_threshold: float = 0.7,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single JSON file and create semantic chunks.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        similarity_threshold: Similarity threshold for splitting\n",
    "        overlap_percent: Percentage of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = data.get('text', '')\n",
    "    metadata = data.get('metadata', {})\n",
    "    \n",
    "    # Create semantic chunks\n",
    "    chunks = semantic_chunk(text, max_chunk_size, similarity_threshold, overlap_percent)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    result = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        result.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk_text,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"source_file\": metadata.get('source_file', ''),\n",
    "            \"source_path\": metadata.get('source_path', '')\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def save_chunks_jsonl(chunks: List[Dict], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Save chunks to JSONL file (one JSON object per line).\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        output_path: Path for output JSONL file\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            json.dump(chunk, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def process_all_json_files(\n",
    "    input_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\text\",\n",
    "    output_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\chunks\",\n",
    "    max_chunk_size: int = 1000,\n",
    "    similarity_threshold: float = 0.7,\n",
    "    overlap_percent: float = 0.1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process all JSON files and create semantic chunks with overlap.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing JSON files\n",
    "        output_dir: Directory to save chunked JSONL files\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        similarity_threshold: Lower = more splits (0.6-0.8 recommended)\n",
    "        overlap_percent: Overlap percentage (0.1 = 10%)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = list(input_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    print(f\"Settings: max_chunk_size={max_chunk_size}, threshold={similarity_threshold}, overlap={overlap_percent*100}%\\n\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            print(f\"Processing: {json_file.name}\")\n",
    "            \n",
    "            # Create semantic chunks\n",
    "            chunks = process_json_file(\n",
    "                json_file, max_chunk_size, similarity_threshold, overlap_percent\n",
    "            )\n",
    "            \n",
    "            # Save as JSONL\n",
    "            output_file = output_path / f\"{json_file.stem}_chunks.jsonl\"\n",
    "            save_chunks_jsonl(chunks, output_file)\n",
    "            \n",
    "            total_chunks += len(chunks)\n",
    "            print(f\"  âœ“ Created {len(chunks)} chunks â†’ {output_file.name}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error: {e}\\n\")\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Total chunks created: {total_chunks}\")\n",
    "    print(f\"Files saved to: {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525b16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 JSON files to process\n",
      "Settings: max_chunk_size=1000, threshold=0.7, overlap=10.0%\n",
      "\n",
      "Processing: 1) Temel kavramlar Ã¶nyargÄ±, kalÄ±pyargÄ± ve ayrÄ±mcÄ±lÄ±k.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 183 chunks â†’ 1) Temel kavramlar Ã¶nyargÄ±, kalÄ±pyargÄ± ve ayrÄ±mcÄ±lÄ±k_chunks.jsonl\n",
      "\n",
      "Processing: 10) TÃœRKÄ°YEâ€™DE Ã–RGÃœTLENME Ã–ZGÃœRLÃœÄžÃœNÃœN GENEL GÃ–RÃœNÃœMÃœ-II .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1233 chunks â†’ 10) TÃœRKÄ°YEâ€™DE Ã–RGÃœTLENME Ã–ZGÃœRLÃœÄžÃœNÃœN GENEL GÃ–RÃœNÃœMÃœ-II _chunks.jsonl\n",
      "\n",
      "Processing: 11) Yurttaslik_Alani_Bilgi_Notu_1.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 195 chunks â†’ 11) Yurttaslik_Alani_Bilgi_Notu_1_chunks.jsonl\n",
      "\n",
      "Processing: 12) TERÃ–RLE MÃœCADELEYÄ° ARAÃ‡SALLAÅžTIRMAK.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 370 chunks â†’ 12) TERÃ–RLE MÃœCADELEYÄ° ARAÃ‡SALLAÅžTIRMAK_chunks.jsonl\n",
      "\n",
      "Processing: 13) PROTESTO HAKKINI KORU.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 557 chunks â†’ 13) PROTESTO HAKKINI KORU_chunks.jsonl\n",
      "\n",
      "Processing: 14) KomploTeorileri_AR_23.03.23_web.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 685 chunks â†’ 14) KomploTeorileri_AR_23.03.23_web_chunks.jsonl\n",
      "\n",
      "Processing: 15) Feminist_Hareketin_Gundemleri_.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 260 chunks â†’ 15) Feminist_Hareketin_Gundemleri__chunks.jsonl\n",
      "\n",
      "Processing: 16) Sivil Toplum KuruluÅŸlarÄ±nÄ±n Devlet TarafÄ±ndan FinansmanÄ± Ãœzerine Bir TartÄ±ÅŸma.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 281 chunks â†’ 16) Sivil Toplum KuruluÅŸlarÄ±nÄ±n Devlet TarafÄ±ndan FinansmanÄ± Ãœzerine Bir TartÄ±ÅŸma_chunks.jsonl\n",
      "\n",
      "Processing: 17) GenÃ§lik PolitikalarÄ±nda KarÅŸÄ±laÅŸtÄ±rmalÄ± Bir DeÄŸerlendirme-TÃ¼rkiye ve Finlandiya Ã–rneÄŸi.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 518 chunks â†’ 17) GenÃ§lik PolitikalarÄ±nda KarÅŸÄ±laÅŸtÄ±rmalÄ± Bir DeÄŸerlendirme-TÃ¼rkiye ve Finlandiya Ã–rneÄŸi_chunks.jsonl\n",
      "\n",
      "Processing: 18) Avrupa Konseyi Politik Karar Alma SÃ¼reÃ§lerine Sivil KatÄ±lÄ±m Rehberi Ã‡evirisi.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 175 chunks â†’ 18) Avrupa Konseyi Politik Karar Alma SÃ¼reÃ§lerine Sivil KatÄ±lÄ±m Rehberi Ã‡evirisi_chunks.jsonl\n",
      "\n",
      "Processing: 19) KampÃ¼sten Ã–ÄŸrenci TopluluklarÄ± .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 2210 chunks â†’ 19) KampÃ¼sten Ã–ÄŸrenci TopluluklarÄ± _chunks.jsonl\n",
      "\n",
      "Processing: 2) AyrÄ±mcÄ±lÄ±k ve medya.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 212 chunks â†’ 2) AyrÄ±mcÄ±lÄ±k ve medya_chunks.jsonl\n",
      "\n",
      "Processing: 20) GenÃ§ler Ne(ler) Ä°stiyor_ .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 222 chunks â†’ 20) GenÃ§ler Ne(ler) Ä°stiyor_ _chunks.jsonl\n",
      "\n",
      "Processing: 21) TÃ¼rkiyeâ€™de GenÃ§lik ve Siyaset_ Gelecek Ä°Ã§in NasÄ±l Bir KatÄ±lÄ±m_ .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 642 chunks â†’ 21) TÃ¼rkiyeâ€™de GenÃ§lik ve Siyaset_ Gelecek Ä°Ã§in NasÄ±l Bir KatÄ±lÄ±m_ _chunks.jsonl\n",
      "\n",
      "Processing: 22) GenÃ§lik AraÅŸtÄ±rmalarÄ± Dergisi 13.sayÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 3866 chunks â†’ 22) GenÃ§lik AraÅŸtÄ±rmalarÄ± Dergisi 13.sayÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 23) TÃ¼rkiye_de GenÃ§lik Miti 1980 SonrasÄ± TÃ¼rkiye GenÃ§liÄŸi Ä°letiÅŸim YayÄ±nlarÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 3825 chunks â†’ 23) TÃ¼rkiye_de GenÃ§lik Miti 1980 SonrasÄ± TÃ¼rkiye GenÃ§liÄŸi Ä°letiÅŸim YayÄ±nlarÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 24) TÃ¼rkiyeâ€™nin GenÃ§liÄŸi AraÅŸtÄ±rmasÄ± Raporu -SODEV- .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 69 chunks â†’ 24) TÃ¼rkiyeâ€™nin GenÃ§liÄŸi AraÅŸtÄ±rmasÄ± Raporu -SODEV- _chunks.jsonl\n",
      "\n",
      "Processing: 25) TÃ¼rkiyeâ€™de GenÃ§lerin GÃ¼vencesizliÄŸi_ Ã‡alÄ±ÅŸma, GeÃ§im ve YaÅŸam AlgÄ±sÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1064 chunks â†’ 25) TÃ¼rkiyeâ€™de GenÃ§lerin GÃ¼vencesizliÄŸi_ Ã‡alÄ±ÅŸma, GeÃ§im ve YaÅŸam AlgÄ±sÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 26) Toplumun BoÄŸaziÃ§i Ãœniversitesi OlaylarÄ±na BakÄ±ÅŸÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 251 chunks â†’ 26) Toplumun BoÄŸaziÃ§i Ãœniversitesi OlaylarÄ±na BakÄ±ÅŸÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 27) KÃ¼rt GenÃ§lerâ€™20 Benzerlikler Farklar DeÄŸiÅŸimler.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 785 chunks â†’ 27) KÃ¼rt GenÃ§lerâ€™20 Benzerlikler Farklar DeÄŸiÅŸimler_chunks.jsonl\n",
      "\n",
      "Processing: 28) NEET GenÃ§ler AraÅŸtÄ±rmasÄ± â€“ NEET GenÃ§lerin Ä°nsan Onuruna YaraÅŸÄ±r YaÅŸam SÃ¼rme HakkÄ±na EriÅŸimi.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1162 chunks â†’ 28) NEET GenÃ§ler AraÅŸtÄ±rmasÄ± â€“ NEET GenÃ§lerin Ä°nsan Onuruna YaraÅŸÄ±r YaÅŸam SÃ¼rme HakkÄ±na EriÅŸimi_chunks.jsonl\n",
      "\n",
      "Processing: 29) TGSP TÃ¼rkiyeâ€™nin GenÃ§leri AraÅŸtÄ±rmasÄ±.pdf.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 396 chunks â†’ 29) TGSP TÃ¼rkiyeâ€™nin GenÃ§leri AraÅŸtÄ±rmasÄ±.pdf_chunks.jsonl\n",
      "\n",
      "Processing: 3) Toplumsal Cinsiyete DayalÄ± AyrÄ±mcÄ±lÄ±k.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 201 chunks â†’ 3) Toplumsal Cinsiyete DayalÄ± AyrÄ±mcÄ±lÄ±k_chunks.jsonl\n",
      "\n",
      "Processing: 30) TOG GenÃ§lik Ã‡alÄ±ÅŸmasÄ±nÄ±n Toplumsal KatÄ±lÄ±ma Etkisi AraÅŸtÄ±rmasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1317 chunks â†’ 30) TOG GenÃ§lik Ã‡alÄ±ÅŸmasÄ±nÄ±n Toplumsal KatÄ±lÄ±ma Etkisi AraÅŸtÄ±rmasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 31) TÃ¼rkiyeâ€™de GenÃ§lerin Ä°yi Olma Hali Saha AraÅŸtÄ±rmasÄ± BulgularÄ±- HABÄ°TAT- .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1689 chunks â†’ 31) TÃ¼rkiyeâ€™de GenÃ§lerin Ä°yi Olma Hali Saha AraÅŸtÄ±rmasÄ± BulgularÄ±- HABÄ°TAT- _chunks.jsonl\n",
      "\n",
      "Processing: 32) TÃ¼rkiye GenÃ§lik AraÅŸtÄ±rmasÄ± 2021.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 8093 chunks â†’ 32) TÃ¼rkiye GenÃ§lik AraÅŸtÄ±rmasÄ± 2021_chunks.jsonl\n",
      "\n",
      "Processing: 33) TÃ¼rkiyeâ€™nin GenÃ§liÄŸi AraÅŸtÄ±rmasÄ± Raporu -SODEV- 2020.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 66 chunks â†’ 33) TÃ¼rkiyeâ€™nin GenÃ§liÄŸi AraÅŸtÄ±rmasÄ± Raporu -SODEV- 2020_chunks.jsonl\n",
      "\n",
      "Processing: 34)UluslararasÄ± Af Ã–rgÃ¼tÃ¼.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1387 chunks â†’ 34)UluslararasÄ± Af Ã–rgÃ¼tÃ¼_chunks.jsonl\n",
      "\n",
      "Processing: 37) Perspectives on Youth Participation.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 381 chunks â†’ 37) Perspectives on Youth Participation_chunks.jsonl\n",
      "\n",
      "Processing: 38) Young peopleâ€™s right to assemble peacefully.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 526 chunks â†’ 38) Young peopleâ€™s right to assemble peacefully_chunks.jsonl\n",
      "\n",
      "Processing: 39) Shrinking democratic civic space for youth.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 487 chunks â†’ 39) Shrinking democratic civic space for youth_chunks.jsonl\n",
      "\n",
      "Processing: 4) UluslararasÄ± Af Ã–rgÃ¼tÃ¼ Raporu 2021-2022 Avrupa ve Orta Asya DeÄŸerlendirmesi(sayfa 46-54).json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 220 chunks â†’ 4) UluslararasÄ± Af Ã–rgÃ¼tÃ¼ Raporu 2021-2022 Avrupa ve Orta Asya DeÄŸerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "\n",
      "Processing: 40) TÃ¼rkiyeâ€™de GenÃ§ Ä°ntiharlarÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1343 chunks â†’ 40) TÃ¼rkiyeâ€™de GenÃ§ Ä°ntiharlarÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 41) TÃ¼rkiyeâ€™de GenÃ§ Ä°ntiharlarÄ± Politika Ã–nerileri.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 43 chunks â†’ 41) TÃ¼rkiyeâ€™de GenÃ§ Ä°ntiharlarÄ± Politika Ã–nerileri_chunks.jsonl\n",
      "\n",
      "Processing: 42) TÃ¼rkiye_de ifade ve medya Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ ve insan haklarÄ± savunucularÄ± ile sivil toplumun durumu hakkÄ±ndaki memorandum.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 368 chunks â†’ 42) TÃ¼rkiye_de ifade ve medya Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ ve insan haklarÄ± savunucularÄ± ile sivil toplumun durumu hakkÄ±ndaki memorandum_chunks.jsonl\n",
      "\n",
      "Processing: 43) TÃ¼rkiyeâ€™deki GenÃ§lik Ã–rgÃ¼tlerinin Ä°htiyaÃ§ Analizi Raporu 2025.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1033 chunks â†’ 43) TÃ¼rkiyeâ€™deki GenÃ§lik Ã–rgÃ¼tlerinin Ä°htiyaÃ§ Analizi Raporu 2025_chunks.jsonl\n",
      "\n",
      "Processing: 44)Toplumsal DeÄŸerler ve GenÃ§lik AraÅŸtÄ±rma Raporu.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 363 chunks â†’ 44)Toplumsal DeÄŸerler ve GenÃ§lik AraÅŸtÄ±rma Raporu_chunks.jsonl\n",
      "\n",
      "Processing: 45) GenÃ§lerin Politik Tercihleri AraÅŸtÄ±rmasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 922 chunks â†’ 45) GenÃ§lerin Politik Tercihleri AraÅŸtÄ±rmasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 46)GenÃ§lik AraÅŸtÄ±rmalarÄ± Dergisi 35.sayÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 3582 chunks â†’ 46)GenÃ§lik AraÅŸtÄ±rmalarÄ± Dergisi 35.sayÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 47)GenÃ§ler iÃ§in nasÄ±l bir kent_.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 198 chunks â†’ 47)GenÃ§ler iÃ§in nasÄ±l bir kent__chunks.jsonl\n",
      "\n",
      "Processing: 48) TGSP GenÃ§lerin GÃ¶nÃ¼llÃ¼lÃ¼k AlgÄ±sÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 184 chunks â†’ 48) TGSP GenÃ§lerin GÃ¶nÃ¼llÃ¼lÃ¼k AlgÄ±sÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 49) Yerel Yonetimlere Iliskin PolitikaBelgesi.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 94 chunks â†’ 49) Yerel Yonetimlere Iliskin PolitikaBelgesi_chunks.jsonl\n",
      "\n",
      "Processing: 5) Paralel Kariyer ArayÄ±ÅŸÄ±nÄ±n Nedenleri Ispartaâ€™da Faaliyet GÃ¶steren STKâ€™larda Bir AraÅŸtÄ±rma.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 249 chunks â†’ 5) Paralel Kariyer ArayÄ±ÅŸÄ±nÄ±n Nedenleri Ispartaâ€™da Faaliyet GÃ¶steren STKâ€™larda Bir AraÅŸtÄ±rma_chunks.jsonl\n",
      "\n",
      "Processing: 50) MilliyetÃ§iliÄŸin DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve GenÃ§ YÃ¼zleri.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 2300 chunks â†’ 50) MilliyetÃ§iliÄŸin DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve GenÃ§ YÃ¼zleri_chunks.jsonl\n",
      "\n",
      "Processing: 51)GenÃ§lik AraÅŸtÄ±rmasÄ± - TÃ¼rkiye 2024.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 606 chunks â†’ 51)GenÃ§lik AraÅŸtÄ±rmasÄ± - TÃ¼rkiye 2024_chunks.jsonl\n",
      "\n",
      "Processing: 52)CORE GenÃ§lerin SeÃ§imi.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 257 chunks â†’ 52)CORE GenÃ§lerin SeÃ§imi_chunks.jsonl\n",
      "\n",
      "Processing: 53) GenÃ§lerin GÃ¼Ã§lendirilmesine YÃ¶nelik HarcamalarÄ± Ä°zleme KÄ±lavuzu.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 4 chunks â†’ 53) GenÃ§lerin GÃ¼Ã§lendirilmesine YÃ¶nelik HarcamalarÄ± Ä°zleme KÄ±lavuzu_chunks.jsonl\n",
      "\n",
      "Processing: 54) GenÃ§lerin Siyasi KatÄ±lÄ±mÄ± - Åžebeke 1.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 3049 chunks â†’ 54) GenÃ§lerin Siyasi KatÄ±lÄ±mÄ± - Åžebeke 1_chunks.jsonl\n",
      "\n",
      "Processing: 55) TÃ¼rkiye_de GenÃ§lerin KatÄ±lÄ±mÄ± - Åžebeke.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1083 chunks â†’ 55) TÃ¼rkiye_de GenÃ§lerin KatÄ±lÄ±mÄ± - Åžebeke_chunks.jsonl\n",
      "\n",
      "Processing: 56) COVID-19 Pandemisi SÃ¼recinde GenÃ§lerin Ä°yilik Halinin Belirlenmesi AraÅŸtÄ±rmasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1930 chunks â†’ 56) COVID-19 Pandemisi SÃ¼recinde GenÃ§lerin Ä°yilik Halinin Belirlenmesi AraÅŸtÄ±rmasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 57) KONDA - HafÄ±za Merkezi GencÌ§lerin IÌ‡nsan HaklarÄ± AlgÄ±sÄ±_.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1010 chunks â†’ 57) KONDA - HafÄ±za Merkezi GencÌ§lerin IÌ‡nsan HaklarÄ± AlgÄ±sÄ±__chunks.jsonl\n",
      "\n",
      "Processing: 58) TÃ¼rkiye_de GenÃ§lik Ã‡alÄ±ÅŸmasÄ± ve PolitikasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 5330 chunks â†’ 58) TÃ¼rkiye_de GenÃ§lik Ã‡alÄ±ÅŸmasÄ± ve PolitikasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 59) LGBTIÌ‡ gencÌ§ler gencÌ§lik merkezlerinde ne istiyor_.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 215 chunks â†’ 59) LGBTIÌ‡ gencÌ§ler gencÌ§lik merkezlerinde ne istiyor__chunks.jsonl\n",
      "\n",
      "Processing: 6) Sivil Toplum Ã–rgÃ¼tlerinde Profesyonel ve GÃ¶nÃ¼llÃ¼ Ã‡alÄ±ÅŸma Ä°liÅŸkileri Tehditler Ve FÄ±rsatlar.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 254 chunks â†’ 6) Sivil Toplum Ã–rgÃ¼tlerinde Profesyonel ve GÃ¶nÃ¼llÃ¼ Ã‡alÄ±ÅŸma Ä°liÅŸkileri Tehditler Ve FÄ±rsatlar_chunks.jsonl\n",
      "\n",
      "Processing: 60) GencÌ§ KadÄ±nlarÄ±n Karar Alma MekanizmalarÄ±na KatÄ±lÄ±mÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 162 chunks â†’ 60) GencÌ§ KadÄ±nlarÄ±n Karar Alma MekanizmalarÄ±na KatÄ±lÄ±mÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1138 chunks â†’ 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi_chunks.jsonl\n",
      "\n",
      "Processing: 62) TGSP TuÌˆrkiye_nin GencÌ§leri YurtdÄ±sÌ§Ä± AlgÄ±sÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 55 chunks â†’ 62) TGSP TuÌˆrkiye_nin GencÌ§leri YurtdÄ±sÌ§Ä± AlgÄ±sÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 63) TGSP TuÌˆrkiye_nin GencÌ§leri DindarlÄ±k AlgÄ±sÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 196 chunks â†’ 63) TGSP TuÌˆrkiye_nin GencÌ§leri DindarlÄ±k AlgÄ±sÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 64) TGSP TuÌˆrkiye_nin GencÌ§leri YuÌˆksekoÌˆgÌ†renim AlgÄ±sÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 265 chunks â†’ 64) TGSP TuÌˆrkiye_nin GencÌ§leri YuÌˆksekoÌˆgÌ†renim AlgÄ±sÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 65 ) TuÌˆrkiye GencÌ§lik ArasÌ§tÄ±rmasÄ± 2023.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 244 chunks â†’ 65 ) TuÌˆrkiye GencÌ§lik ArasÌ§tÄ±rmasÄ± 2023_chunks.jsonl\n",
      "\n",
      "Processing: 66) SODEV Genclik Arastirmasi Raporu 2021.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 61 chunks â†’ 66) SODEV Genclik Arastirmasi Raporu 2021_chunks.jsonl\n",
      "\n",
      "Processing: 67) SERHAT TRA2 GencÌ§lik ArasÌ§tÄ±rmasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 5485 chunks â†’ 67) SERHAT TRA2 GencÌ§lik ArasÌ§tÄ±rmasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 68) Ä°PA Ä°stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1288 chunks â†’ 68) Ä°PA Ä°stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim_chunks.jsonl\n",
      "\n",
      "Processing: 69) Ä°PA Ãœniversite Mezunu Ev GenÃ§leri AraÅŸtÄ±rmasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 55 chunks â†’ 69) Ä°PA Ãœniversite Mezunu Ev GenÃ§leri AraÅŸtÄ±rmasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 7) eÅŸitsiz demokrasiler.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 348 chunks â†’ 7) eÅŸitsiz demokrasiler_chunks.jsonl\n",
      "\n",
      "Processing: 70) IPM TuÌˆrkiye_de GencÌ§lerin YurtdÄ±sÌ§Ä±nda YasÌ§ama IÌ‡stegÌ†i.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 104 chunks â†’ 70) IPM TuÌˆrkiye_de GencÌ§lerin YurtdÄ±sÌ§Ä±nda YasÌ§ama IÌ‡stegÌ†i_chunks.jsonl\n",
      "\n",
      "Processing: 71) IÌ‡PM TuÌˆrkiye_de AkÄ±llÄ± KentlesÌ§me ve GencÌ§lik PolitikalarÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 110 chunks â†’ 71) IÌ‡PM TuÌˆrkiye_de AkÄ±llÄ± KentlesÌ§me ve GencÌ§lik PolitikalarÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 72) TUÌˆSES GencÌ§lerin Cinsel SagÌ†lÄ±k ve UÌˆreme SaÄŸlÄ±ÄŸÄ± ArasÌ§tÄ±rmasÄ±.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1452 chunks â†’ 72) TUÌˆSES GencÌ§lerin Cinsel SagÌ†lÄ±k ve UÌˆreme SaÄŸlÄ±ÄŸÄ± ArasÌ§tÄ±rmasÄ±_chunks.jsonl\n",
      "\n",
      "Processing: 73) FES Youth Study Southeast Europe .json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1920 chunks â†’ 73) FES Youth Study Southeast Europe _chunks.jsonl\n",
      "\n",
      "Processing: 74) Biarada IÌ‡stanbulâ€™da GencÌ§lik, Kent YurttasÌ§lÄ±gÌ†Ä± ve Yerel YoÌˆnetim.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1647 chunks â†’ 74) Biarada IÌ‡stanbulâ€™da GencÌ§lik, Kent YurttasÌ§lÄ±gÌ†Ä± ve Yerel YoÌˆnetim_chunks.jsonl\n",
      "\n",
      "Processing: 75) TuÌˆrkiye GencÌ§lik ArasÌ§tÄ±rmasÄ± OÌˆzet Bulgular 2023.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 186 chunks â†’ 75) TuÌˆrkiye GencÌ§lik ArasÌ§tÄ±rmasÄ± OÌˆzet Bulgular 2023_chunks.jsonl\n",
      "\n",
      "Processing: 76) Toplum CÌ§alÄ±sÌ§masÄ± EnstituÌˆsuÌˆ Kim Bu GencÌ§ler_.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 101 chunks â†’ 76) Toplum CÌ§alÄ±sÌ§masÄ± EnstituÌˆsuÌˆ Kim Bu GencÌ§ler__chunks.jsonl\n",
      "\n",
      "Processing: 77) KAOS GL LGBTIÌ‡+ OÌˆgÌ†renciler.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1210 chunks â†’ 77) KAOS GL LGBTIÌ‡+ OÌˆgÌ†renciler_chunks.jsonl\n",
      "\n",
      "Processing: 78) TOG UÌˆniversiteli GencÌ§lerin IÌ‡htiyacÌ§larÄ± ArasÌ§tÄ±rmasÄ± 2024.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 288 chunks â†’ 78) TOG UÌˆniversiteli GencÌ§lerin IÌ‡htiyacÌ§larÄ± ArasÌ§tÄ±rmasÄ± 2024_chunks.jsonl\n",
      "\n",
      "Processing: 79) Haberlerdeki Ãœniversite 2022.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 538 chunks â†’ 79) Haberlerdeki Ãœniversite 2022_chunks.jsonl\n",
      "\n",
      "Processing: 8) genc-oy-strateji-rapor.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1264 chunks â†’ 8) genc-oy-strateji-rapor_chunks.jsonl\n",
      "\n",
      "Processing: 80) TOG GencÌ§lerin IÌ‡htiyacÌ§larÄ± ArasÌ§tÄ±rmasÄ± 2022.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 128 chunks â†’ 80) TOG GencÌ§lerin IÌ‡htiyacÌ§larÄ± ArasÌ§tÄ±rmasÄ± 2022_chunks.jsonl\n",
      "\n",
      "Processing: 81) Yereliz GENCÌ§LIÌ‡K ALANINDA CÌ§ALISÌ§AN SIÌ‡VIÌ‡L TOPLUM OÌˆRGUÌˆTLERIÌ‡ IÌ‡CÌ§IÌ‡N YEREL SAVUNUCULUK REHBERIÌ‡.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 345 chunks â†’ 81) Yereliz GENCÌ§LIÌ‡K ALANINDA CÌ§ALISÌ§AN SIÌ‡VIÌ‡L TOPLUM OÌˆRGUÌˆTLERIÌ‡ IÌ‡CÌ§IÌ‡N YEREL SAVUNUCULUK REHBERIÌ‡_chunks.jsonl\n",
      "\n",
      "Processing: 82) KONDA Barometre 2024.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 258 chunks â†’ 82) KONDA Barometre 2024_chunks.jsonl\n",
      "\n",
      "Processing: 83) OECD Youth Policy Toolkit.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 3058 chunks â†’ 83) OECD Youth Policy Toolkit_chunks.jsonl\n",
      "\n",
      "Processing: 84) TIÌ‡P_li OÌˆgÌ†renciler BarÄ±nma Raporu 2023.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 706 chunks â†’ 84) TIÌ‡P_li OÌˆgÌ†renciler BarÄ±nma Raporu 2023_chunks.jsonl\n",
      "\n",
      "Processing: 85) ILO Global Employement Trends for Youth 2020.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 2338 chunks â†’ 85) ILO Global Employement Trends for Youth 2020_chunks.jsonl\n",
      "\n",
      "Processing: 86) GoFor Hangi GenÃ§_.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 505 chunks â†’ 86) GoFor Hangi GenÃ§__chunks.jsonl\n",
      "\n",
      "Processing: 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 63 chunks â†’ 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu_chunks.jsonl\n",
      "\n",
      "Processing: 88) TuÌˆrkiye_de NEET UÌˆzerine YapÄ±lmÄ±sÌ§ Ã‡alÄ±ÅŸmalara Ä°liÅŸkin Bir DegÌ†erlendirme 2024.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 501 chunks â†’ 88) TuÌˆrkiye_de NEET UÌˆzerine YapÄ±lmÄ±sÌ§ Ã‡alÄ±ÅŸmalara Ä°liÅŸkin Bir DegÌ†erlendirme 2024_chunks.jsonl\n",
      "\n",
      "Processing: 89) UNFPA IÌ‡statisliklerle GencÌ§lik.json\n",
      "  âœ“ Created 1 chunks â†’ 89) UNFPA IÌ‡statisliklerle GencÌ§lik_chunks.jsonl\n",
      "\n",
      "Processing: 9) Politik Karar Verme SÃ¼reÃ§lerine Etkili ve AnlamlÄ± KATILIM HAKKI ve MEKANÄ°ZMALAR.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 1786 chunks â†’ 9) Politik Karar Verme SÃ¼reÃ§lerine Etkili ve AnlamlÄ± KATILIM HAKKI ve MEKANÄ°ZMALAR_chunks.jsonl\n",
      "\n",
      "Processing: 90) FES Genclerin GoÌˆzuÌˆnden Dindar-SekuÌˆler Eksenli KutuplasÌ§ma.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 3230 chunks â†’ 90) FES Genclerin GoÌˆzuÌˆnden Dindar-SekuÌˆler Eksenli KutuplasÌ§ma_chunks.jsonl\n",
      "\n",
      "Processing: 91) Veriler.json\n",
      "  Calculating sentence similarities... âœ“\n",
      "  âœ“ Created 92 chunks â†’ 91) Veriler_chunks.jsonl\n",
      "\n",
      "Processing complete!\n",
      "Total chunks created: 89073\n",
      "Files saved to: C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\chunks\n"
     ]
    }
   ],
   "source": [
    "# Run the chunker\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSON files with semantic chunking and 10% overlap\n",
    "    process_all_json_files(\n",
    "        max_chunk_size=1000,\n",
    "        similarity_threshold=0.7,  # Adjust between 0.6-0.8\n",
    "        overlap_percent=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43978d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de154d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\chunks\n",
      "Total files: 89 | Total chunks: 89073\n",
      "\n",
      "Global character counts:\n",
      "  count=89073 min=2 max=926151 mean=185.1 median=122.0 p95=430.0 std=4007.4\n",
      "Global word counts:\n",
      "  count=89073 min=1 max=4202 mean=22.4 median=16.0 p95=57.0 std=39.5\n",
      "\n",
      "Top-5 files by mean char_count:\n",
      "  53) GenÃ§lerin GÃ¼Ã§lendirilmesine YÃ¶nelik HarcamalarÄ± Ä°zleme KÄ±lavuzu_chunks.jsonl mean=500441.0 (chunks=4)\n",
      "  89) UNFPA IÌ‡statisliklerle GencÌ§lik_chunks.jsonl             mean=3269.0 (chunks=1)\n",
      "  64) TGSP TuÌˆrkiye_nin GencÌ§leri YuÌˆksekoÌˆgÌ†renim AlgÄ±sÄ±_chunks.jsonl mean=547.0 (chunks=265)\n",
      "  63) TGSP TuÌˆrkiye_nin GencÌ§leri DindarlÄ±k AlgÄ±sÄ±_chunks.jsonl mean=522.0 (chunks=196)\n",
      "  48) TGSP GenÃ§lerin GÃ¶nÃ¼llÃ¼lÃ¼k AlgÄ±sÄ±_chunks.jsonl            mean=468.1 (chunks=184)\n"
     ]
    }
   ],
   "source": [
    "# Utilities to compute chunk size statistics over a folder of JSONL chunk files\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import math\n",
    "\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read a JSONL file and return a list of dicts.\"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed lines but continue\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "\n",
    "def summarize(values: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Return summary stats for a numeric list.\"\"\"\n",
    "    if not values:\n",
    "        return {\n",
    "            'count': 0,\n",
    "            'min': 0,\n",
    "            'max': 0,\n",
    "            'mean': 0,\n",
    "            'median': 0,\n",
    "            'p95': 0,\n",
    "            'std': 0,\n",
    "        }\n",
    "    n = len(values)\n",
    "    values_sorted = sorted(values)\n",
    "    total = sum(values)\n",
    "    mean = total / n\n",
    "    # Median\n",
    "    if n % 2 == 1:\n",
    "        median = values_sorted[n // 2]\n",
    "    else:\n",
    "        median = (values_sorted[n // 2 - 1] + values_sorted[n // 2]) / 2\n",
    "    # p95\n",
    "    p95_index = min(n - 1, max(0, int(math.ceil(0.95 * n) - 1)))\n",
    "    p95 = values_sorted[p95_index]\n",
    "    # std (population std)\n",
    "    var = sum((x - mean) ** 2 for x in values) / n\n",
    "    std = math.sqrt(var)\n",
    "    return {\n",
    "        'count': n,\n",
    "        'min': values_sorted[0],\n",
    "        'max': values_sorted[-1],\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'p95': p95,\n",
    "        'std': std,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_chunks_folder(folder: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Compute per-file and global stats for chunk JSONL files in a folder.\"\"\"\n",
    "    folder = folder.resolve()\n",
    "    files = sorted(folder.glob('*.jsonl'))\n",
    "    per_file: Dict[str, Any] = {}\n",
    "    all_chars: List[int] = []\n",
    "    all_words: List[int] = []\n",
    "\n",
    "    for fp in files:\n",
    "        rows = read_jsonl(fp)\n",
    "        char_counts = []\n",
    "        word_counts = []\n",
    "        for r in rows:\n",
    "            # prefer explicit fields; fallback to computing from text if missing\n",
    "            if 'char_count' in r and isinstance(r['char_count'], (int, float)):\n",
    "                char_counts.append(int(r['char_count']))\n",
    "            elif 'text' in r:\n",
    "                char_counts.append(len(r['text']))\n",
    "            if 'word_count' in r and isinstance(r['word_count'], (int, float)):\n",
    "                word_counts.append(int(r['word_count']))\n",
    "            elif 'text' in r:\n",
    "                word_counts.append(len(r['text'].split()))\n",
    "        per_file[fp.name] = {\n",
    "            'files_counted': len(rows),\n",
    "            'char_stats': summarize(char_counts),\n",
    "            'word_stats': summarize(word_counts),\n",
    "        }\n",
    "        all_chars.extend(char_counts)\n",
    "        all_words.extend(word_counts)\n",
    "\n",
    "    global_stats = {\n",
    "        'total_files': len(files),\n",
    "        'total_chunks': len(all_chars),\n",
    "        'char_stats': summarize(all_chars),\n",
    "        'word_stats': summarize(all_words),\n",
    "    }\n",
    "    return {\n",
    "        'folder': str(folder),\n",
    "        'global': global_stats,\n",
    "        'per_file': per_file,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_stats_report(report: Dict[str, Any]) -> None:\n",
    "    \"\"\"Pretty-print a compact stats report.\"\"\"\n",
    "    g = report['global']\n",
    "    print(f\"Folder: {report['folder']}\")\n",
    "    print(f\"Total files: {g['total_files']} | Total chunks: {g['total_chunks']}\")\n",
    "    cs = g['char_stats']\n",
    "    ws = g['word_stats']\n",
    "    print(\"\\nGlobal character counts:\")\n",
    "    print(f\"  count={cs['count']} min={cs['min']} max={cs['max']} mean={cs['mean']:.1f} median={cs['median']:.1f} p95={cs['p95']:.1f} std={cs['std']:.1f}\")\n",
    "    print(\"Global word counts:\")\n",
    "    print(f\"  count={ws['count']} min={ws['min']} max={ws['max']} mean={ws['mean']:.1f} median={ws['median']:.1f} p95={ws['p95']:.1f} std={ws['std']:.1f}\")\n",
    "\n",
    "    # Show top-5 largest files by mean char count\n",
    "    print(\"\\nTop-5 files by mean char_count:\")\n",
    "    rows = []\n",
    "    for fn, stats in report['per_file'].items():\n",
    "        rows.append((fn, stats['char_stats']['mean'], stats['files_counted']))\n",
    "    for fn, mean_val, cnt in sorted(rows, key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  {fn:60s} mean={mean_val:.1f} (chunks={cnt})\")\n",
    "\n",
    "\n",
    "def save_stats(report: Dict[str, Any], out_json: Path = None) -> None:\n",
    "    if out_json is None:\n",
    "        return\n",
    "    with out_json.open('w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# Example: analyze the default 'chunks' folder in the repo root\n",
    "# You can change this path if needed.\n",
    "if __name__ == '__main__':\n",
    "    chunks_dir = Path(r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\chunks\")\n",
    "    report = analyze_chunks_folder(chunks_dir)\n",
    "    print_stats_report(report)\n",
    "    # Optionally save to JSON next to the folder\n",
    "    # save_stats(report, chunks_dir / 'chunks_stats.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d4572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
