{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def chunk_by_paragraphs(text: str, max_chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks by paragraphs.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split by double newlines (paragraphs)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "            \n",
    "        para_length = len(para)\n",
    "        \n",
    "        # If single paragraph is too large, split it by sentences\n",
    "        if para_length > max_chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n\\n'.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "            \n",
    "            # Split large paragraph into sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "            temp_chunk = []\n",
    "            temp_length = 0\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if temp_length + len(sentence) > max_chunk_size and temp_chunk:\n",
    "                    chunks.append(' '.join(temp_chunk))\n",
    "                    temp_chunk = [sentence]\n",
    "                    temp_length = len(sentence)\n",
    "                else:\n",
    "                    temp_chunk.append(sentence)\n",
    "                    temp_length += len(sentence)\n",
    "            \n",
    "            if temp_chunk:\n",
    "                chunks.append(' '.join(temp_chunk))\n",
    "        \n",
    "        # If adding paragraph exceeds max size, save current chunk\n",
    "        elif current_length + para_length > max_chunk_size and current_chunk:\n",
    "            chunks.append('\\n\\n'.join(current_chunk))\n",
    "            current_chunk = [para]\n",
    "            current_length = para_length\n",
    "        else:\n",
    "            current_chunk.append(para)\n",
    "            current_length += para_length\n",
    "    \n",
    "    # Add remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_json_file(json_path: Path, max_chunk_size: int = 1000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single JSON file and create chunks.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = data.get('text', '')\n",
    "    metadata = data.get('metadata', {})\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = chunk_by_paragraphs(text, max_chunk_size)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    result = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        result.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk_text,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"source_file\": metadata.get('source_file', ''),\n",
    "            \"source_path\": metadata.get('source_path', '')\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def save_chunks_jsonl(chunks: List[Dict], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Save chunks to JSONL file (one JSON object per line).\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        output_path: Path for output JSONL file\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            json.dump(chunk, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def process_all_json_files(\n",
    "    input_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\text\",\n",
    "    output_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\basic-chunks\",\n",
    "    max_chunk_size: int = 1000\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process all JSON files and create chunked JSONL files.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing JSON files\n",
    "        output_dir: Directory to save chunked JSONL files\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = list(input_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\\n\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            print(f\"Processing: {json_file.name}...\", end=\" \")\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = process_json_file(json_file, max_chunk_size)\n",
    "            \n",
    "            # Save as JSONL\n",
    "            output_file = output_path / f\"{json_file.stem}_chunks.jsonl\"\n",
    "            save_chunks_jsonl(chunks, output_file)\n",
    "            \n",
    "            total_chunks += len(chunks)\n",
    "            print(f\"‚úì Created {len(chunks)} chunks ‚Üí {output_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total chunks created: {total_chunks}\")\n",
    "    print(f\"Files saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525b16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 JSON files to process\n",
      "\n",
      "Processing: 1) Temel kavramlar √∂nyargƒ±, kalƒ±pyargƒ± ve ayrƒ±mcƒ±lƒ±k.json... ‚úì Created 34 chunks ‚Üí 1) Temel kavramlar √∂nyargƒ±, kalƒ±pyargƒ± ve ayrƒ±mcƒ±lƒ±k_chunks.jsonl\n",
      "Processing: 10) T√úRKƒ∞YE‚ÄôDE √ñRG√úTLENME √ñZG√úRL√úƒû√úN√úN GENEL G√ñR√úN√úM√ú-II .json... ‚úì Created 193 chunks ‚Üí 10) T√úRKƒ∞YE‚ÄôDE √ñRG√úTLENME √ñZG√úRL√úƒû√úN√úN GENEL G√ñR√úN√úM√ú-II _chunks.jsonl\n",
      "Processing: 11) Yurttaslik_Alani_Bilgi_Notu_1.json... ‚úì Created 31 chunks ‚Üí 11) Yurttaslik_Alani_Bilgi_Notu_1_chunks.jsonl\n",
      "Processing: 12) TER√ñRLE M√úCADELEYƒ∞ ARA√áSALLA≈ûTIRMAK.json... ‚úì Created 107 chunks ‚Üí 12) TER√ñRLE M√úCADELEYƒ∞ ARA√áSALLA≈ûTIRMAK_chunks.jsonl\n",
      "Processing: 13) PROTESTO HAKKINI KORU.json... ‚úì Created 264 chunks ‚Üí 13) PROTESTO HAKKINI KORU_chunks.jsonl\n",
      "Processing: 14) KomploTeorileri_AR_23.03.23_web.json... ‚úì Created 89 chunks ‚Üí 14) KomploTeorileri_AR_23.03.23_web_chunks.jsonl\n",
      "Processing: 15) Feminist_Hareketin_Gundemleri_.json... ‚úì Created 54 chunks ‚Üí 15) Feminist_Hareketin_Gundemleri__chunks.jsonl\n",
      "Processing: 16) Sivil Toplum Kurulu≈ülarƒ±nƒ±n Devlet Tarafƒ±ndan Finansmanƒ± √úzerine Bir Tartƒ±≈üma.json... ‚úì Created 41 chunks ‚Üí 16) Sivil Toplum Kurulu≈ülarƒ±nƒ±n Devlet Tarafƒ±ndan Finansmanƒ± √úzerine Bir Tartƒ±≈üma_chunks.jsonl\n",
      "Processing: 17) Gen√ßlik Politikalarƒ±nda Kar≈üƒ±la≈ütƒ±rmalƒ± Bir Deƒüerlendirme-T√ºrkiye ve Finlandiya √ñrneƒüi.json... ‚úì Created 86 chunks ‚Üí 17) Gen√ßlik Politikalarƒ±nda Kar≈üƒ±la≈ütƒ±rmalƒ± Bir Deƒüerlendirme-T√ºrkiye ve Finlandiya √ñrneƒüi_chunks.jsonl\n",
      "Processing: 18) Avrupa Konseyi Politik Karar Alma S√ºre√ßlerine Sivil Katƒ±lƒ±m Rehberi √áevirisi.json... ‚úì Created 38 chunks ‚Üí 18) Avrupa Konseyi Politik Karar Alma S√ºre√ßlerine Sivil Katƒ±lƒ±m Rehberi √áevirisi_chunks.jsonl\n",
      "Processing: 19) Kamp√ºsten √ñƒürenci Topluluklarƒ± .json... ‚úì Created 455 chunks ‚Üí 19) Kamp√ºsten √ñƒürenci Topluluklarƒ± _chunks.jsonl\n",
      "Processing: 2) Ayrƒ±mcƒ±lƒ±k ve medya.json... ‚úì Created 45 chunks ‚Üí 2) Ayrƒ±mcƒ±lƒ±k ve medya_chunks.jsonl\n",
      "Processing: 20) Gen√ßler Ne(ler) ƒ∞stiyor_ .json... ‚úì Created 30 chunks ‚Üí 20) Gen√ßler Ne(ler) ƒ∞stiyor_ _chunks.jsonl\n",
      "Processing: 21) T√ºrkiye‚Äôde Gen√ßlik ve Siyaset_ Gelecek ƒ∞√ßin Nasƒ±l Bir Katƒ±lƒ±m_ .json... ‚úì Created 72 chunks ‚Üí 21) T√ºrkiye‚Äôde Gen√ßlik ve Siyaset_ Gelecek ƒ∞√ßin Nasƒ±l Bir Katƒ±lƒ±m_ _chunks.jsonl\n",
      "Processing: 22) Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 13.sayƒ±.json... ‚úì Created 624 chunks ‚Üí 22) Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 13.sayƒ±_chunks.jsonl\n",
      "Processing: 23) T√ºrkiye_de Gen√ßlik Miti 1980 Sonrasƒ± T√ºrkiye Gen√ßliƒüi ƒ∞leti≈üim Yayƒ±nlarƒ±.json... ‚úì Created 485 chunks ‚Üí 23) T√ºrkiye_de Gen√ßlik Miti 1980 Sonrasƒ± T√ºrkiye Gen√ßliƒüi ƒ∞leti≈üim Yayƒ±nlarƒ±_chunks.jsonl\n",
      "Processing: 24) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- .json... ‚úì Created 9 chunks ‚Üí 24) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- _chunks.jsonl\n",
      "Processing: 25) T√ºrkiye‚Äôde Gen√ßlerin G√ºvencesizliƒüi_ √áalƒ±≈üma, Ge√ßim ve Ya≈üam Algƒ±sƒ±.json... ‚úì Created 171 chunks ‚Üí 25) T√ºrkiye‚Äôde Gen√ßlerin G√ºvencesizliƒüi_ √áalƒ±≈üma, Ge√ßim ve Ya≈üam Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 26) Toplumun Boƒüazi√ßi √úniversitesi Olaylarƒ±na Bakƒ±≈üƒ±.json... ‚úì Created 36 chunks ‚Üí 26) Toplumun Boƒüazi√ßi √úniversitesi Olaylarƒ±na Bakƒ±≈üƒ±_chunks.jsonl\n",
      "Processing: 27) K√ºrt Gen√ßler‚Äô20 Benzerlikler Farklar Deƒüi≈üimler.json... ‚úì Created 125 chunks ‚Üí 27) K√ºrt Gen√ßler‚Äô20 Benzerlikler Farklar Deƒüi≈üimler_chunks.jsonl\n",
      "Processing: 28) NEET Gen√ßler Ara≈ütƒ±rmasƒ± ‚Äì NEET Gen√ßlerin ƒ∞nsan Onuruna Yara≈üƒ±r Ya≈üam S√ºrme Hakkƒ±na Eri≈üimi.json... ‚úì Created 281 chunks ‚Üí 28) NEET Gen√ßler Ara≈ütƒ±rmasƒ± ‚Äì NEET Gen√ßlerin ƒ∞nsan Onuruna Yara≈üƒ±r Ya≈üam S√ºrme Hakkƒ±na Eri≈üimi_chunks.jsonl\n",
      "Processing: 29) TGSP T√ºrkiye‚Äônin Gen√ßleri Ara≈ütƒ±rmasƒ±.pdf.json... ‚úì Created 79 chunks ‚Üí 29) TGSP T√ºrkiye‚Äônin Gen√ßleri Ara≈ütƒ±rmasƒ±.pdf_chunks.jsonl\n",
      "Processing: 3) Toplumsal Cinsiyete Dayalƒ± Ayrƒ±mcƒ±lƒ±k.json... ‚úì Created 33 chunks ‚Üí 3) Toplumsal Cinsiyete Dayalƒ± Ayrƒ±mcƒ±lƒ±k_chunks.jsonl\n",
      "Processing: 30) TOG Gen√ßlik √áalƒ±≈ümasƒ±nƒ±n Toplumsal Katƒ±lƒ±ma Etkisi Ara≈ütƒ±rmasƒ±.json... ‚úì Created 182 chunks ‚Üí 30) TOG Gen√ßlik √áalƒ±≈ümasƒ±nƒ±n Toplumsal Katƒ±lƒ±ma Etkisi Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 31) T√ºrkiye‚Äôde Gen√ßlerin ƒ∞yi Olma Hali Saha Ara≈ütƒ±rmasƒ± Bulgularƒ±- HABƒ∞TAT- .json... ‚úì Created 230 chunks ‚Üí 31) T√ºrkiye‚Äôde Gen√ßlerin ƒ∞yi Olma Hali Saha Ara≈ütƒ±rmasƒ± Bulgularƒ±- HABƒ∞TAT- _chunks.jsonl\n",
      "Processing: 32) T√ºrkiye Gen√ßlik Ara≈ütƒ±rmasƒ± 2021.json... ‚úì Created 833 chunks ‚Üí 32) T√ºrkiye Gen√ßlik Ara≈ütƒ±rmasƒ± 2021_chunks.jsonl\n",
      "Processing: 33) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- 2020.json... ‚úì Created 10 chunks ‚Üí 33) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- 2020_chunks.jsonl\n",
      "Processing: 34)Uluslararasƒ± Af √ñrg√ºt√º.json... ‚úì Created 227 chunks ‚Üí 34)Uluslararasƒ± Af √ñrg√ºt√º_chunks.jsonl\n",
      "Processing: 37) Perspectives on Youth Participation.json... ‚úì Created 58 chunks ‚Üí 37) Perspectives on Youth Participation_chunks.jsonl\n",
      "Processing: 38) Young people‚Äôs right to assemble peacefully.json... ‚úì Created 84 chunks ‚Üí 38) Young people‚Äôs right to assemble peacefully_chunks.jsonl\n",
      "Processing: 39) Shrinking democratic civic space for youth.json... ‚úì Created 79 chunks ‚Üí 39) Shrinking democratic civic space for youth_chunks.jsonl\n",
      "Processing: 4) Uluslararasƒ± Af √ñrg√ºt√º Raporu 2021-2022 Avrupa ve Orta Asya Deƒüerlendirmesi(sayfa 46-54).json... ‚úì Created 35 chunks ‚Üí 4) Uluslararasƒ± Af √ñrg√ºt√º Raporu 2021-2022 Avrupa ve Orta Asya Deƒüerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "Processing: 40) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ±.json... ‚úì Created 170 chunks ‚Üí 40) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ±_chunks.jsonl\n",
      "Processing: 41) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ± Politika √ñnerileri.json... ‚úì Created 11 chunks ‚Üí 41) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ± Politika √ñnerileri_chunks.jsonl\n",
      "Processing: 42) T√ºrkiye_de ifade ve medya √∂zg√ºrl√ºƒü√º ve insan haklarƒ± savunucularƒ± ile sivil toplumun durumu hakkƒ±ndaki memorandum.json... ‚úì Created 68 chunks ‚Üí 42) T√ºrkiye_de ifade ve medya √∂zg√ºrl√ºƒü√º ve insan haklarƒ± savunucularƒ± ile sivil toplumun durumu hakkƒ±ndaki memorandum_chunks.jsonl\n",
      "Processing: 43) T√ºrkiye‚Äôdeki Gen√ßlik √ñrg√ºtlerinin ƒ∞htiya√ß Analizi Raporu 2025.json... ‚úì Created 169 chunks ‚Üí 43) T√ºrkiye‚Äôdeki Gen√ßlik √ñrg√ºtlerinin ƒ∞htiya√ß Analizi Raporu 2025_chunks.jsonl\n",
      "Processing: 44)Toplumsal Deƒüerler ve Gen√ßlik Ara≈ütƒ±rma Raporu.json... ‚úì Created 56 chunks ‚Üí 44)Toplumsal Deƒüerler ve Gen√ßlik Ara≈ütƒ±rma Raporu_chunks.jsonl\n",
      "Processing: 45) Gen√ßlerin Politik Tercihleri Ara≈ütƒ±rmasƒ±.json... ‚úì Created 187 chunks ‚Üí 45) Gen√ßlerin Politik Tercihleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 46)Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 35.sayƒ±.json... ‚úì Created 361 chunks ‚Üí 46)Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 35.sayƒ±_chunks.jsonl\n",
      "Processing: 47)Gen√ßler i√ßin nasƒ±l bir kent_.json... ‚úì Created 79 chunks ‚Üí 29) TGSP T√ºrkiye‚Äônin Gen√ßleri Ara≈ütƒ±rmasƒ±.pdf_chunks.jsonl\n",
      "Processing: 3) Toplumsal Cinsiyete Dayalƒ± Ayrƒ±mcƒ±lƒ±k.json... ‚úì Created 33 chunks ‚Üí 3) Toplumsal Cinsiyete Dayalƒ± Ayrƒ±mcƒ±lƒ±k_chunks.jsonl\n",
      "Processing: 30) TOG Gen√ßlik √áalƒ±≈ümasƒ±nƒ±n Toplumsal Katƒ±lƒ±ma Etkisi Ara≈ütƒ±rmasƒ±.json... ‚úì Created 182 chunks ‚Üí 30) TOG Gen√ßlik √áalƒ±≈ümasƒ±nƒ±n Toplumsal Katƒ±lƒ±ma Etkisi Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 31) T√ºrkiye‚Äôde Gen√ßlerin ƒ∞yi Olma Hali Saha Ara≈ütƒ±rmasƒ± Bulgularƒ±- HABƒ∞TAT- .json... ‚úì Created 230 chunks ‚Üí 31) T√ºrkiye‚Äôde Gen√ßlerin ƒ∞yi Olma Hali Saha Ara≈ütƒ±rmasƒ± Bulgularƒ±- HABƒ∞TAT- _chunks.jsonl\n",
      "Processing: 32) T√ºrkiye Gen√ßlik Ara≈ütƒ±rmasƒ± 2021.json... ‚úì Created 833 chunks ‚Üí 32) T√ºrkiye Gen√ßlik Ara≈ütƒ±rmasƒ± 2021_chunks.jsonl\n",
      "Processing: 33) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- 2020.json... ‚úì Created 10 chunks ‚Üí 33) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- 2020_chunks.jsonl\n",
      "Processing: 34)Uluslararasƒ± Af √ñrg√ºt√º.json... ‚úì Created 227 chunks ‚Üí 34)Uluslararasƒ± Af √ñrg√ºt√º_chunks.jsonl\n",
      "Processing: 37) Perspectives on Youth Participation.json... ‚úì Created 58 chunks ‚Üí 37) Perspectives on Youth Participation_chunks.jsonl\n",
      "Processing: 38) Young people‚Äôs right to assemble peacefully.json... ‚úì Created 84 chunks ‚Üí 38) Young people‚Äôs right to assemble peacefully_chunks.jsonl\n",
      "Processing: 39) Shrinking democratic civic space for youth.json... ‚úì Created 79 chunks ‚Üí 39) Shrinking democratic civic space for youth_chunks.jsonl\n",
      "Processing: 4) Uluslararasƒ± Af √ñrg√ºt√º Raporu 2021-2022 Avrupa ve Orta Asya Deƒüerlendirmesi(sayfa 46-54).json... ‚úì Created 35 chunks ‚Üí 4) Uluslararasƒ± Af √ñrg√ºt√º Raporu 2021-2022 Avrupa ve Orta Asya Deƒüerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "Processing: 40) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ±.json... ‚úì Created 170 chunks ‚Üí 40) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ±_chunks.jsonl\n",
      "Processing: 41) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ± Politika √ñnerileri.json... ‚úì Created 11 chunks ‚Üí 41) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ± Politika √ñnerileri_chunks.jsonl\n",
      "Processing: 42) T√ºrkiye_de ifade ve medya √∂zg√ºrl√ºƒü√º ve insan haklarƒ± savunucularƒ± ile sivil toplumun durumu hakkƒ±ndaki memorandum.json... ‚úì Created 68 chunks ‚Üí 42) T√ºrkiye_de ifade ve medya √∂zg√ºrl√ºƒü√º ve insan haklarƒ± savunucularƒ± ile sivil toplumun durumu hakkƒ±ndaki memorandum_chunks.jsonl\n",
      "Processing: 43) T√ºrkiye‚Äôdeki Gen√ßlik √ñrg√ºtlerinin ƒ∞htiya√ß Analizi Raporu 2025.json... ‚úì Created 169 chunks ‚Üí 43) T√ºrkiye‚Äôdeki Gen√ßlik √ñrg√ºtlerinin ƒ∞htiya√ß Analizi Raporu 2025_chunks.jsonl\n",
      "Processing: 44)Toplumsal Deƒüerler ve Gen√ßlik Ara≈ütƒ±rma Raporu.json... ‚úì Created 56 chunks ‚Üí 44)Toplumsal Deƒüerler ve Gen√ßlik Ara≈ütƒ±rma Raporu_chunks.jsonl\n",
      "Processing: 45) Gen√ßlerin Politik Tercihleri Ara≈ütƒ±rmasƒ±.json... ‚úì Created 187 chunks ‚Üí 45) Gen√ßlerin Politik Tercihleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 46)Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 35.sayƒ±.json... ‚úì Created 361 chunks ‚Üí 46)Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 35.sayƒ±_chunks.jsonl\n",
      "Processing: 47)Gen√ßler i√ßin nasƒ±l bir kent_.json... ‚úì Created 40 chunks ‚Üí 47)Gen√ßler i√ßin nasƒ±l bir kent__chunks.jsonl\n",
      "Processing: 48) TGSP Gen√ßlerin G√∂n√ºll√ºl√ºk Algƒ±sƒ±.json... ‚úì Created 93 chunks ‚Üí 48) TGSP Gen√ßlerin G√∂n√ºll√ºl√ºk Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 49) Yerel Yonetimlere Iliskin PolitikaBelgesi.json... ‚úì Created 26 chunks ‚Üí 49) Yerel Yonetimlere Iliskin PolitikaBelgesi_chunks.jsonl\n",
      "Processing: 5) Paralel Kariyer Arayƒ±≈üƒ±nƒ±n Nedenleri Isparta‚Äôda Faaliyet G√∂steren STK‚Äôlarda Bir Ara≈ütƒ±rma.json... ‚úì Created 37 chunks ‚Üí 5) Paralel Kariyer Arayƒ±≈üƒ±nƒ±n Nedenleri Isparta‚Äôda Faaliyet G√∂steren STK‚Äôlarda Bir Ara≈ütƒ±rma_chunks.jsonl\n",
      "Processing: 50) Milliyet√ßiliƒüin D√∂n√º≈ü√ºm√º ve Gen√ß Y√ºzleri.json... ‚úì Created 273 chunks ‚Üí 50) Milliyet√ßiliƒüin D√∂n√º≈ü√ºm√º ve Gen√ß Y√ºzleri_chunks.jsonl\n",
      "Processing: 51)Gen√ßlik Ara≈ütƒ±rmasƒ± - T√ºrkiye 2024.json... ‚úì Created 121 chunks ‚Üí 51)Gen√ßlik Ara≈ütƒ±rmasƒ± - T√ºrkiye 2024_chunks.jsonl\n",
      "Processing: 52)CORE Gen√ßlerin Se√ßimi.json... ‚úì Created 86 chunks ‚Üí 52)CORE Gen√ßlerin Se√ßimi_chunks.jsonl\n",
      "Processing: 53) Gen√ßlerin G√º√ßlendirilmesine Y√∂nelik Harcamalarƒ± ƒ∞zleme Kƒ±lavuzu.json... ‚úì Created 4 chunks ‚Üí 53) Gen√ßlerin G√º√ßlendirilmesine Y√∂nelik Harcamalarƒ± ƒ∞zleme Kƒ±lavuzu_chunks.jsonl\n",
      "Processing: 54) Gen√ßlerin Siyasi Katƒ±lƒ±mƒ± - ≈ûebeke 1.json... ‚úì Created 495 chunks ‚Üí 54) Gen√ßlerin Siyasi Katƒ±lƒ±mƒ± - ≈ûebeke 1_chunks.jsonl\n",
      "Processing: 55) T√ºrkiye_de Gen√ßlerin Katƒ±lƒ±mƒ± - ≈ûebeke.json... ‚úì Created 174 chunks ‚Üí 55) T√ºrkiye_de Gen√ßlerin Katƒ±lƒ±mƒ± - ≈ûebeke_chunks.jsonl\n",
      "Processing: 56) COVID-19 Pandemisi S√ºrecinde Gen√ßlerin ƒ∞yilik Halinin Belirlenmesi Ara≈ütƒ±rmasƒ±.json... ‚úì Created 413 chunks ‚Üí 56) COVID-19 Pandemisi S√ºrecinde Gen√ßlerin ƒ∞yilik Halinin Belirlenmesi Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 57) KONDA - Hafƒ±za Merkezi GencÃßlerin IÃánsan Haklarƒ± Algƒ±sƒ±_.json... ‚úì Created 221 chunks ‚Üí 57) KONDA - Hafƒ±za Merkezi GencÃßlerin IÃánsan Haklarƒ± Algƒ±sƒ±__chunks.jsonl\n",
      "Processing: 58) T√ºrkiye_de Gen√ßlik √áalƒ±≈ümasƒ± ve Politikasƒ±.json... ‚úì Created 40 chunks ‚Üí 47)Gen√ßler i√ßin nasƒ±l bir kent__chunks.jsonl\n",
      "Processing: 48) TGSP Gen√ßlerin G√∂n√ºll√ºl√ºk Algƒ±sƒ±.json... ‚úì Created 93 chunks ‚Üí 48) TGSP Gen√ßlerin G√∂n√ºll√ºl√ºk Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 49) Yerel Yonetimlere Iliskin PolitikaBelgesi.json... ‚úì Created 26 chunks ‚Üí 49) Yerel Yonetimlere Iliskin PolitikaBelgesi_chunks.jsonl\n",
      "Processing: 5) Paralel Kariyer Arayƒ±≈üƒ±nƒ±n Nedenleri Isparta‚Äôda Faaliyet G√∂steren STK‚Äôlarda Bir Ara≈ütƒ±rma.json... ‚úì Created 37 chunks ‚Üí 5) Paralel Kariyer Arayƒ±≈üƒ±nƒ±n Nedenleri Isparta‚Äôda Faaliyet G√∂steren STK‚Äôlarda Bir Ara≈ütƒ±rma_chunks.jsonl\n",
      "Processing: 50) Milliyet√ßiliƒüin D√∂n√º≈ü√ºm√º ve Gen√ß Y√ºzleri.json... ‚úì Created 273 chunks ‚Üí 50) Milliyet√ßiliƒüin D√∂n√º≈ü√ºm√º ve Gen√ß Y√ºzleri_chunks.jsonl\n",
      "Processing: 51)Gen√ßlik Ara≈ütƒ±rmasƒ± - T√ºrkiye 2024.json... ‚úì Created 121 chunks ‚Üí 51)Gen√ßlik Ara≈ütƒ±rmasƒ± - T√ºrkiye 2024_chunks.jsonl\n",
      "Processing: 52)CORE Gen√ßlerin Se√ßimi.json... ‚úì Created 86 chunks ‚Üí 52)CORE Gen√ßlerin Se√ßimi_chunks.jsonl\n",
      "Processing: 53) Gen√ßlerin G√º√ßlendirilmesine Y√∂nelik Harcamalarƒ± ƒ∞zleme Kƒ±lavuzu.json... ‚úì Created 4 chunks ‚Üí 53) Gen√ßlerin G√º√ßlendirilmesine Y√∂nelik Harcamalarƒ± ƒ∞zleme Kƒ±lavuzu_chunks.jsonl\n",
      "Processing: 54) Gen√ßlerin Siyasi Katƒ±lƒ±mƒ± - ≈ûebeke 1.json... ‚úì Created 495 chunks ‚Üí 54) Gen√ßlerin Siyasi Katƒ±lƒ±mƒ± - ≈ûebeke 1_chunks.jsonl\n",
      "Processing: 55) T√ºrkiye_de Gen√ßlerin Katƒ±lƒ±mƒ± - ≈ûebeke.json... ‚úì Created 174 chunks ‚Üí 55) T√ºrkiye_de Gen√ßlerin Katƒ±lƒ±mƒ± - ≈ûebeke_chunks.jsonl\n",
      "Processing: 56) COVID-19 Pandemisi S√ºrecinde Gen√ßlerin ƒ∞yilik Halinin Belirlenmesi Ara≈ütƒ±rmasƒ±.json... ‚úì Created 413 chunks ‚Üí 56) COVID-19 Pandemisi S√ºrecinde Gen√ßlerin ƒ∞yilik Halinin Belirlenmesi Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 57) KONDA - Hafƒ±za Merkezi GencÃßlerin IÃánsan Haklarƒ± Algƒ±sƒ±_.json... ‚úì Created 221 chunks ‚Üí 57) KONDA - Hafƒ±za Merkezi GencÃßlerin IÃánsan Haklarƒ± Algƒ±sƒ±__chunks.jsonl\n",
      "Processing: 58) T√ºrkiye_de Gen√ßlik √áalƒ±≈ümasƒ± ve Politikasƒ±.json... ‚úì Created 1146 chunks ‚Üí 58) T√ºrkiye_de Gen√ßlik √áalƒ±≈ümasƒ± ve Politikasƒ±_chunks.jsonl\n",
      "Processing: 59) LGBTIÃá gencÃßler gencÃßlik merkezlerinde ne istiyor_.json... ‚úì Created 35 chunks ‚Üí 59) LGBTIÃá gencÃßler gencÃßlik merkezlerinde ne istiyor__chunks.jsonl\n",
      "Processing: 6) Sivil Toplum √ñrg√ºtlerinde Profesyonel ve G√∂n√ºll√º √áalƒ±≈üma ƒ∞li≈ükileri Tehditler Ve Fƒ±rsatlar.json... ‚úì Created 43 chunks ‚Üí 6) Sivil Toplum √ñrg√ºtlerinde Profesyonel ve G√∂n√ºll√º √áalƒ±≈üma ƒ∞li≈ükileri Tehditler Ve Fƒ±rsatlar_chunks.jsonl\n",
      "Processing: 60) GencÃß Kadƒ±nlarƒ±n Karar Alma Mekanizmalarƒ±na Katƒ±lƒ±mƒ±.json... ‚úì Created 22 chunks ‚Üí 60) GencÃß Kadƒ±nlarƒ±n Karar Alma Mekanizmalarƒ±na Katƒ±lƒ±mƒ±_chunks.jsonl\n",
      "Processing: 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi.json... ‚úì Created 269 chunks ‚Üí 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi_chunks.jsonl\n",
      "Processing: 62) TGSP TuÃàrkiye_nin GencÃßleri Yurtdƒ±sÃßƒ± Algƒ±sƒ±.json... ‚úì Created 11 chunks ‚Üí 62) TGSP TuÃàrkiye_nin GencÃßleri Yurtdƒ±sÃßƒ± Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 63) TGSP TuÃàrkiye_nin GencÃßleri Dindarlƒ±k Algƒ±sƒ±.json... ‚úì Created 106 chunks ‚Üí 63) TGSP TuÃàrkiye_nin GencÃßleri Dindarlƒ±k Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 64) TGSP TuÃàrkiye_nin GencÃßleri YuÃàksekoÃàgÃÜrenim Algƒ±sƒ±.json... ‚úì Created 161 chunks ‚Üí 64) TGSP TuÃàrkiye_nin GencÃßleri YuÃàksekoÃàgÃÜrenim Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 65 ) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± 2023.json... ‚úì Created 22 chunks ‚Üí 65 ) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± 2023_chunks.jsonl\n",
      "Processing: 66) SODEV Genclik Arastirmasi Raporu 2021.json... ‚úì Created 14 chunks ‚Üí 66) SODEV Genclik Arastirmasi Raporu 2021_chunks.jsonl\n",
      "Processing: 67) SERHAT TRA2 GencÃßlik ArasÃßtƒ±rmasƒ±.json... ‚úì Created 825 chunks ‚Üí 67) SERHAT TRA2 GencÃßlik ArasÃßtƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 68) ƒ∞PA ƒ∞stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim.json... ‚úì Created 229 chunks ‚Üí 68) ƒ∞PA ƒ∞stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim_chunks.jsonl\n",
      "Processing: 69) ƒ∞PA √úniversite Mezunu Ev Gen√ßleri Ara≈ütƒ±rmasƒ±.json... ‚úì Created 10 chunks ‚Üí 69) ƒ∞PA √úniversite Mezunu Ev Gen√ßleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 7) e≈üitsiz demokrasiler.json... ‚úì Created 54 chunks ‚Üí 7) e≈üitsiz demokrasiler_chunks.jsonl\n",
      "Processing: 70) IPM TuÃàrkiye_de GencÃßlerin Yurtdƒ±sÃßƒ±nda YasÃßama IÃástegÃÜi.json... ‚úì Created 20 chunks ‚Üí 70) IPM TuÃàrkiye_de GencÃßlerin Yurtdƒ±sÃßƒ±nda YasÃßama IÃástegÃÜi_chunks.jsonl\n",
      "Processing: 71) IÃáPM TuÃàrkiye_de Akƒ±llƒ± KentlesÃßme ve GencÃßlik Politikalarƒ±.json... ‚úì Created 37 chunks ‚Üí 71) IÃáPM TuÃàrkiye_de Akƒ±llƒ± KentlesÃßme ve GencÃßlik Politikalarƒ±_chunks.jsonl\n",
      "Processing: 72) TUÃàSES GencÃßlerin Cinsel SagÃÜlƒ±k ve UÃàreme Saƒülƒ±ƒüƒ± ArasÃßtƒ±rmasƒ±.json... ‚úì Created 178 chunks ‚Üí 72) TUÃàSES GencÃßlerin Cinsel SagÃÜlƒ±k ve UÃàreme Saƒülƒ±ƒüƒ± ArasÃßtƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 73) FES Youth Study Southeast Europe .json... ‚úì Created 329 chunks ‚Üí 73) FES Youth Study Southeast Europe _chunks.jsonl\n",
      "Processing: 74) Biarada IÃástanbul‚Äôda GencÃßlik, Kent YurttasÃßlƒ±gÃÜƒ± ve Yerel YoÃànetim.json... ‚úì Created 196 chunks ‚Üí 74) Biarada IÃástanbul‚Äôda GencÃßlik, Kent YurttasÃßlƒ±gÃÜƒ± ve Yerel YoÃànetim_chunks.jsonl\n",
      "Processing: 75) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± OÃàzet Bulgular 2023.json... ‚úì Created 33 chunks ‚Üí 75) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± OÃàzet Bulgular 2023_chunks.jsonl\n",
      "Processing: 76) Toplum CÃßalƒ±sÃßmasƒ± EnstituÃàsuÃà Kim Bu GencÃßler_.json... ‚úì Created 16 chunks ‚Üí 76) Toplum CÃßalƒ±sÃßmasƒ± EnstituÃàsuÃà Kim Bu GencÃßler__chunks.jsonl\n",
      "Processing: 77) KAOS GL LGBTIÃá+ OÃàgÃÜrenciler.json... ‚úì Created 210 chunks ‚Üí 77) KAOS GL LGBTIÃá+ OÃàgÃÜrenciler_chunks.jsonl\n",
      "Processing: 78) TOG UÃàniversiteli GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2024.json... ‚úì Created 50 chunks ‚Üí 78) TOG UÃàniversiteli GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2024_chunks.jsonl\n",
      "Processing: 79) Haberlerdeki √úniversite 2022.json... ‚úì Created 78 chunks ‚Üí 79) Haberlerdeki √úniversite 2022_chunks.jsonl\n",
      "Processing: 8) genc-oy-strateji-rapor.json... ‚úì Created 145 chunks ‚Üí 8) genc-oy-strateji-rapor_chunks.jsonl\n",
      "Processing: 80) TOG GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2022.json... ‚úì Created 18 chunks ‚Üí 80) TOG GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2022_chunks.jsonl\n",
      "Processing: 81) Yereliz GENCÃßLIÃáK ALANINDA CÃßALISÃßAN SIÃáVIÃáL TOPLUM OÃàRGUÃàTLERIÃá IÃáCÃßIÃáN YEREL SAVUNUCULUK REHBERIÃá.json... ‚úì Created 1146 chunks ‚Üí 58) T√ºrkiye_de Gen√ßlik √áalƒ±≈ümasƒ± ve Politikasƒ±_chunks.jsonl\n",
      "Processing: 59) LGBTIÃá gencÃßler gencÃßlik merkezlerinde ne istiyor_.json... ‚úì Created 35 chunks ‚Üí 59) LGBTIÃá gencÃßler gencÃßlik merkezlerinde ne istiyor__chunks.jsonl\n",
      "Processing: 6) Sivil Toplum √ñrg√ºtlerinde Profesyonel ve G√∂n√ºll√º √áalƒ±≈üma ƒ∞li≈ükileri Tehditler Ve Fƒ±rsatlar.json... ‚úì Created 43 chunks ‚Üí 6) Sivil Toplum √ñrg√ºtlerinde Profesyonel ve G√∂n√ºll√º √áalƒ±≈üma ƒ∞li≈ükileri Tehditler Ve Fƒ±rsatlar_chunks.jsonl\n",
      "Processing: 60) GencÃß Kadƒ±nlarƒ±n Karar Alma Mekanizmalarƒ±na Katƒ±lƒ±mƒ±.json... ‚úì Created 22 chunks ‚Üí 60) GencÃß Kadƒ±nlarƒ±n Karar Alma Mekanizmalarƒ±na Katƒ±lƒ±mƒ±_chunks.jsonl\n",
      "Processing: 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi.json... ‚úì Created 269 chunks ‚Üí 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi_chunks.jsonl\n",
      "Processing: 62) TGSP TuÃàrkiye_nin GencÃßleri Yurtdƒ±sÃßƒ± Algƒ±sƒ±.json... ‚úì Created 11 chunks ‚Üí 62) TGSP TuÃàrkiye_nin GencÃßleri Yurtdƒ±sÃßƒ± Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 63) TGSP TuÃàrkiye_nin GencÃßleri Dindarlƒ±k Algƒ±sƒ±.json... ‚úì Created 106 chunks ‚Üí 63) TGSP TuÃàrkiye_nin GencÃßleri Dindarlƒ±k Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 64) TGSP TuÃàrkiye_nin GencÃßleri YuÃàksekoÃàgÃÜrenim Algƒ±sƒ±.json... ‚úì Created 161 chunks ‚Üí 64) TGSP TuÃàrkiye_nin GencÃßleri YuÃàksekoÃàgÃÜrenim Algƒ±sƒ±_chunks.jsonl\n",
      "Processing: 65 ) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± 2023.json... ‚úì Created 22 chunks ‚Üí 65 ) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± 2023_chunks.jsonl\n",
      "Processing: 66) SODEV Genclik Arastirmasi Raporu 2021.json... ‚úì Created 14 chunks ‚Üí 66) SODEV Genclik Arastirmasi Raporu 2021_chunks.jsonl\n",
      "Processing: 67) SERHAT TRA2 GencÃßlik ArasÃßtƒ±rmasƒ±.json... ‚úì Created 825 chunks ‚Üí 67) SERHAT TRA2 GencÃßlik ArasÃßtƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 68) ƒ∞PA ƒ∞stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim.json... ‚úì Created 229 chunks ‚Üí 68) ƒ∞PA ƒ∞stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim_chunks.jsonl\n",
      "Processing: 69) ƒ∞PA √úniversite Mezunu Ev Gen√ßleri Ara≈ütƒ±rmasƒ±.json... ‚úì Created 10 chunks ‚Üí 69) ƒ∞PA √úniversite Mezunu Ev Gen√ßleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 7) e≈üitsiz demokrasiler.json... ‚úì Created 54 chunks ‚Üí 7) e≈üitsiz demokrasiler_chunks.jsonl\n",
      "Processing: 70) IPM TuÃàrkiye_de GencÃßlerin Yurtdƒ±sÃßƒ±nda YasÃßama IÃástegÃÜi.json... ‚úì Created 20 chunks ‚Üí 70) IPM TuÃàrkiye_de GencÃßlerin Yurtdƒ±sÃßƒ±nda YasÃßama IÃástegÃÜi_chunks.jsonl\n",
      "Processing: 71) IÃáPM TuÃàrkiye_de Akƒ±llƒ± KentlesÃßme ve GencÃßlik Politikalarƒ±.json... ‚úì Created 37 chunks ‚Üí 71) IÃáPM TuÃàrkiye_de Akƒ±llƒ± KentlesÃßme ve GencÃßlik Politikalarƒ±_chunks.jsonl\n",
      "Processing: 72) TUÃàSES GencÃßlerin Cinsel SagÃÜlƒ±k ve UÃàreme Saƒülƒ±ƒüƒ± ArasÃßtƒ±rmasƒ±.json... ‚úì Created 178 chunks ‚Üí 72) TUÃàSES GencÃßlerin Cinsel SagÃÜlƒ±k ve UÃàreme Saƒülƒ±ƒüƒ± ArasÃßtƒ±rmasƒ±_chunks.jsonl\n",
      "Processing: 73) FES Youth Study Southeast Europe .json... ‚úì Created 329 chunks ‚Üí 73) FES Youth Study Southeast Europe _chunks.jsonl\n",
      "Processing: 74) Biarada IÃástanbul‚Äôda GencÃßlik, Kent YurttasÃßlƒ±gÃÜƒ± ve Yerel YoÃànetim.json... ‚úì Created 196 chunks ‚Üí 74) Biarada IÃástanbul‚Äôda GencÃßlik, Kent YurttasÃßlƒ±gÃÜƒ± ve Yerel YoÃànetim_chunks.jsonl\n",
      "Processing: 75) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± OÃàzet Bulgular 2023.json... ‚úì Created 33 chunks ‚Üí 75) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± OÃàzet Bulgular 2023_chunks.jsonl\n",
      "Processing: 76) Toplum CÃßalƒ±sÃßmasƒ± EnstituÃàsuÃà Kim Bu GencÃßler_.json... ‚úì Created 16 chunks ‚Üí 76) Toplum CÃßalƒ±sÃßmasƒ± EnstituÃàsuÃà Kim Bu GencÃßler__chunks.jsonl\n",
      "Processing: 77) KAOS GL LGBTIÃá+ OÃàgÃÜrenciler.json... ‚úì Created 210 chunks ‚Üí 77) KAOS GL LGBTIÃá+ OÃàgÃÜrenciler_chunks.jsonl\n",
      "Processing: 78) TOG UÃàniversiteli GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2024.json... ‚úì Created 50 chunks ‚Üí 78) TOG UÃàniversiteli GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2024_chunks.jsonl\n",
      "Processing: 79) Haberlerdeki √úniversite 2022.json... ‚úì Created 78 chunks ‚Üí 79) Haberlerdeki √úniversite 2022_chunks.jsonl\n",
      "Processing: 8) genc-oy-strateji-rapor.json... ‚úì Created 145 chunks ‚Üí 8) genc-oy-strateji-rapor_chunks.jsonl\n",
      "Processing: 80) TOG GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2022.json... ‚úì Created 18 chunks ‚Üí 80) TOG GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2022_chunks.jsonl\n",
      "Processing: 81) Yereliz GENCÃßLIÃáK ALANINDA CÃßALISÃßAN SIÃáVIÃáL TOPLUM OÃàRGUÃàTLERIÃá IÃáCÃßIÃáN YEREL SAVUNUCULUK REHBERIÃá.json... ‚úì Created 66 chunks ‚Üí 81) Yereliz GENCÃßLIÃáK ALANINDA CÃßALISÃßAN SIÃáVIÃáL TOPLUM OÃàRGUÃàTLERIÃá IÃáCÃßIÃáN YEREL SAVUNUCULUK REHBERIÃá_chunks.jsonl\n",
      "Processing: 82) KONDA Barometre 2024.json... ‚úì Created 41 chunks ‚Üí 82) KONDA Barometre 2024_chunks.jsonl\n",
      "Processing: 83) OECD Youth Policy Toolkit.json... ‚úì Created 618 chunks ‚Üí 83) OECD Youth Policy Toolkit_chunks.jsonl\n",
      "Processing: 84) TIÃáP_li OÃàgÃÜrenciler Barƒ±nma Raporu 2023.json... ‚úì Created 100 chunks ‚Üí 84) TIÃáP_li OÃàgÃÜrenciler Barƒ±nma Raporu 2023_chunks.jsonl\n",
      "Processing: 85) ILO Global Employement Trends for Youth 2020.json... ‚úì Created 468 chunks ‚Üí 85) ILO Global Employement Trends for Youth 2020_chunks.jsonl\n",
      "Processing: 86) GoFor Hangi Gen√ß_.json... ‚úì Created 97 chunks ‚Üí 86) GoFor Hangi Gen√ß__chunks.jsonl\n",
      "Processing: 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu.json... ‚úì Created 15 chunks ‚Üí 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu_chunks.jsonl\n",
      "Processing: 88) TuÃàrkiye_de NEET UÃàzerine Yapƒ±lmƒ±sÃß √áalƒ±≈ümalara ƒ∞li≈ükin Bir DegÃÜerlendirme 2024.json... ‚úì Created 71 chunks ‚Üí 88) TuÃàrkiye_de NEET UÃàzerine Yapƒ±lmƒ±sÃß √áalƒ±≈ümalara ƒ∞li≈ükin Bir DegÃÜerlendirme 2024_chunks.jsonl\n",
      "Processing: 89) UNFPA IÃástatisliklerle GencÃßlik.json... ‚úì Created 1 chunks ‚Üí 89) UNFPA IÃástatisliklerle GencÃßlik_chunks.jsonl\n",
      "Processing: 9) Politik Karar Verme S√ºre√ßlerine Etkili ve Anlamlƒ± KATILIM HAKKI ve MEKANƒ∞ZMALAR.json... ‚úì Created 275 chunks ‚Üí 9) Politik Karar Verme S√ºre√ßlerine Etkili ve Anlamlƒ± KATILIM HAKKI ve MEKANƒ∞ZMALAR_chunks.jsonl\n",
      "Processing: 90) FES Genclerin GoÃàzuÃànden Dindar-SekuÃàler Eksenli KutuplasÃßma.json... ‚úì Created 453 chunks ‚Üí 90) FES Genclerin GoÃàzuÃànden Dindar-SekuÃàler Eksenli KutuplasÃßma_chunks.jsonl\n",
      "Processing: 91) Veriler.json... ‚úì Created 19 chunks ‚Üí 91) Veriler_chunks.jsonl\n",
      "\n",
      "Processing complete!\n",
      "Total chunks created: 14606\n",
      "Files saved to: C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\basic-chunks\n",
      "‚úì Created 66 chunks ‚Üí 81) Yereliz GENCÃßLIÃáK ALANINDA CÃßALISÃßAN SIÃáVIÃáL TOPLUM OÃàRGUÃàTLERIÃá IÃáCÃßIÃáN YEREL SAVUNUCULUK REHBERIÃá_chunks.jsonl\n",
      "Processing: 82) KONDA Barometre 2024.json... ‚úì Created 41 chunks ‚Üí 82) KONDA Barometre 2024_chunks.jsonl\n",
      "Processing: 83) OECD Youth Policy Toolkit.json... ‚úì Created 618 chunks ‚Üí 83) OECD Youth Policy Toolkit_chunks.jsonl\n",
      "Processing: 84) TIÃáP_li OÃàgÃÜrenciler Barƒ±nma Raporu 2023.json... ‚úì Created 100 chunks ‚Üí 84) TIÃáP_li OÃàgÃÜrenciler Barƒ±nma Raporu 2023_chunks.jsonl\n",
      "Processing: 85) ILO Global Employement Trends for Youth 2020.json... ‚úì Created 468 chunks ‚Üí 85) ILO Global Employement Trends for Youth 2020_chunks.jsonl\n",
      "Processing: 86) GoFor Hangi Gen√ß_.json... ‚úì Created 97 chunks ‚Üí 86) GoFor Hangi Gen√ß__chunks.jsonl\n",
      "Processing: 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu.json... ‚úì Created 15 chunks ‚Üí 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu_chunks.jsonl\n",
      "Processing: 88) TuÃàrkiye_de NEET UÃàzerine Yapƒ±lmƒ±sÃß √áalƒ±≈ümalara ƒ∞li≈ükin Bir DegÃÜerlendirme 2024.json... ‚úì Created 71 chunks ‚Üí 88) TuÃàrkiye_de NEET UÃàzerine Yapƒ±lmƒ±sÃß √áalƒ±≈ümalara ƒ∞li≈ükin Bir DegÃÜerlendirme 2024_chunks.jsonl\n",
      "Processing: 89) UNFPA IÃástatisliklerle GencÃßlik.json... ‚úì Created 1 chunks ‚Üí 89) UNFPA IÃástatisliklerle GencÃßlik_chunks.jsonl\n",
      "Processing: 9) Politik Karar Verme S√ºre√ßlerine Etkili ve Anlamlƒ± KATILIM HAKKI ve MEKANƒ∞ZMALAR.json... ‚úì Created 275 chunks ‚Üí 9) Politik Karar Verme S√ºre√ßlerine Etkili ve Anlamlƒ± KATILIM HAKKI ve MEKANƒ∞ZMALAR_chunks.jsonl\n",
      "Processing: 90) FES Genclerin GoÃàzuÃànden Dindar-SekuÃàler Eksenli KutuplasÃßma.json... ‚úì Created 453 chunks ‚Üí 90) FES Genclerin GoÃàzuÃànden Dindar-SekuÃàler Eksenli KutuplasÃßma_chunks.jsonl\n",
      "Processing: 91) Veriler.json... ‚úì Created 19 chunks ‚Üí 91) Veriler_chunks.jsonl\n",
      "\n",
      "Processing complete!\n",
      "Total chunks created: 14606\n",
      "Files saved to: C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\basic-chunks\n"
     ]
    }
   ],
   "source": [
    "# Run the chunker\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSON files and create chunks\n",
    "    process_all_json_files(max_chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43978d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d33674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED VERSION - Much faster overlap chunking\n",
    "from collections import deque\n",
    "\n",
    "def chunk_by_paragraphs_optimized(text: str, max_chunk_size: int = 1000, overlap_ratio: float = 0.1) -> List[str]:\n",
    "    \"\"\"\n",
    "    Optimized version: Split text into chunks by paragraphs with overlap.\n",
    "    Uses sliding window approach instead of list insertions for O(n) complexity.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        overlap_ratio: Fraction of chunk to overlap (default 0.1 = 10%)\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks with overlap\n",
    "    \"\"\"\n",
    "    # Split by double newlines (paragraphs)\n",
    "    paragraphs = [para.strip() for para in text.split('\\n\\n') if para.strip()]\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    overlap_size = int(max_chunk_size * overlap_ratio)\n",
    "    \n",
    "    # Pre-calculate paragraph lengths to avoid repeated len() calls\n",
    "    para_lengths = [len(para) for para in paragraphs]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(paragraphs):\n",
    "        current_chunk_paras = []\n",
    "        current_length = 0\n",
    "        start_idx = i\n",
    "        \n",
    "        # Build current chunk\n",
    "        while i < len(paragraphs) and current_length + para_lengths[i] <= max_chunk_size:\n",
    "            current_chunk_paras.append(paragraphs[i])\n",
    "            current_length += para_lengths[i]\n",
    "            i += 1\n",
    "        \n",
    "        # Handle oversized single paragraph\n",
    "        if not current_chunk_paras and i < len(paragraphs):\n",
    "            oversized_para = paragraphs[i]\n",
    "            sentence_chunks = _split_oversized_paragraph_optimized(oversized_para, max_chunk_size, overlap_size)\n",
    "            chunks.extend(sentence_chunks)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        if current_chunk_paras:\n",
    "            chunk_text = '\\n\\n'.join(current_chunk_paras)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            # Calculate overlap for next chunk using sliding window\n",
    "            if i < len(paragraphs):\n",
    "                overlap_start = _find_overlap_start_optimized(\n",
    "                    current_chunk_paras, para_lengths[start_idx:i], overlap_size\n",
    "                )\n",
    "                if overlap_start is not None:\n",
    "                    i = start_idx + overlap_start  # Slide back to overlap position\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _split_oversized_paragraph_optimized(para: str, max_chunk_size: int, overlap_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Optimized helper to split oversized paragraphs into sentences with overlap.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "    sentence_lengths = [len(sent) for sent in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(sentences):\n",
    "        current_sentences = []\n",
    "        current_length = 0\n",
    "        start_idx = i\n",
    "        \n",
    "        # Build sentence chunk\n",
    "        while i < len(sentences) and current_length + sentence_lengths[i] <= max_chunk_size:\n",
    "            current_sentences.append(sentences[i])\n",
    "            current_length += sentence_lengths[i]\n",
    "            i += 1\n",
    "        \n",
    "        if current_sentences:\n",
    "            chunks.append(' '.join(current_sentences))\n",
    "            \n",
    "            # Calculate sentence-level overlap\n",
    "            if i < len(sentences):\n",
    "                overlap_start = _find_sentence_overlap_start(\n",
    "                    current_sentences, sentence_lengths[start_idx:i], overlap_size\n",
    "                )\n",
    "                if overlap_start is not None:\n",
    "                    i = start_idx + overlap_start\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _find_overlap_start_optimized(paragraphs: List[str], lengths: List[int], target_overlap: int) -> int:\n",
    "    \"\"\"\n",
    "    Find the starting index for overlap using reverse accumulation.\n",
    "    Returns the paragraph index to start the next chunk from.\n",
    "    \"\"\"\n",
    "    if not paragraphs or target_overlap <= 0:\n",
    "        return None\n",
    "    \n",
    "    accumulated_length = 0\n",
    "    \n",
    "    # Work backwards from the end\n",
    "    for i in range(len(paragraphs) - 1, -1, -1):\n",
    "        if accumulated_length + lengths[i] <= target_overlap:\n",
    "            accumulated_length += lengths[i]\n",
    "        else:\n",
    "            # If we can't fit the whole paragraph, check if we can fit part of it\n",
    "            remaining_space = target_overlap - accumulated_length\n",
    "            if remaining_space > 50:  # Minimum meaningful overlap\n",
    "                return i  # Start from this paragraph (will be partially included)\n",
    "            elif i < len(paragraphs) - 1:\n",
    "                return i + 1  # Start from next paragraph\n",
    "            else:\n",
    "                return None\n",
    "    \n",
    "    return 0 if accumulated_length > 0 else None\n",
    "\n",
    "\n",
    "def _find_sentence_overlap_start(sentences: List[str], lengths: List[int], target_overlap: int) -> int:\n",
    "    \"\"\"\n",
    "    Find the starting sentence index for overlap.\n",
    "    \"\"\"\n",
    "    if not sentences or target_overlap <= 0:\n",
    "        return None\n",
    "    \n",
    "    accumulated_length = 0\n",
    "    \n",
    "    for i in range(len(sentences) - 1, -1, -1):\n",
    "        if accumulated_length + lengths[i] <= target_overlap:\n",
    "            accumulated_length += lengths[i]\n",
    "        else:\n",
    "            return i + 1 if i < len(sentences) - 1 else None\n",
    "    \n",
    "    return 0 if accumulated_length > 0 else None\n",
    "\n",
    "\n",
    "# Optimized processing functions\n",
    "def process_json_file_optimized(json_path: Path, max_chunk_size: int = 1000, overlap_ratio: float = 0.1) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Optimized version of process_json_file.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = data.get('text', '')\n",
    "    metadata = data.get('metadata', {})\n",
    "    \n",
    "    # Use optimized chunking\n",
    "    chunks = chunk_by_paragraphs_optimized(text, max_chunk_size, overlap_ratio)\n",
    "    \n",
    "    # Pre-allocate result list for better performance\n",
    "    result = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        # Cache split for word count to avoid repeated splitting\n",
    "        words = chunk_text.split()\n",
    "        result.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk_text,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(words),\n",
    "            \"source_file\": metadata.get('source_file', ''),\n",
    "            \"source_path\": metadata.get('source_path', '')\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def process_all_json_files_optimized(\n",
    "    input_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\text\",\n",
    "    output_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\basic-chunks\",\n",
    "    max_chunk_size: int = 1000,\n",
    "    overlap_ratio: float = 0.1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Optimized version with better performance and memory usage.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = list(input_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    print(f\"Chunk size: {max_chunk_size}, Overlap: {overlap_ratio*100:.1f}%\\n\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            print(f\"Processing: {json_file.name}...\", end=\" \")\n",
    "            \n",
    "            # Use optimized processing\n",
    "            chunks = process_json_file_optimized(json_file, max_chunk_size, overlap_ratio)\n",
    "            \n",
    "            # Save as JSONL\n",
    "            output_file = output_path / f\"{json_file.stem}_chunks.jsonl\"\n",
    "            save_chunks_jsonl(chunks, output_file)\n",
    "            \n",
    "            total_chunks += len(chunks)\n",
    "            print(f\"‚úì {len(chunks)} chunks ‚Üí {output_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Processing complete!\")\n",
    "    print(f\"üìä Total chunks created: {total_chunks}\")\n",
    "    print(f\"üíæ Files saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "# Performance comparison function\n",
    "def compare_performance():\n",
    "    \"\"\"\n",
    "    Compare performance between original and optimized versions.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Sample text for testing\n",
    "    sample_text = \"\"\"\n",
    "    This is a sample paragraph for testing purposes. It contains multiple sentences to simulate real document processing.\n",
    "    \n",
    "    This is another paragraph that we'll use to test the chunking algorithms. We want to see how they handle overlapping content.\n",
    "    \n",
    "    The third paragraph adds more content to make the test more realistic. Performance differences should become apparent with larger texts.\n",
    "    \n",
    "    Final paragraph to complete our sample text. This should be enough content to demonstrate the algorithmic differences.\n",
    "    \"\"\" * 50  # Multiply to make it larger\n",
    "    \n",
    "    print(\"üîç Performance Comparison Test\")\n",
    "    print(f\"Sample text length: {len(sample_text)} characters\\n\")\n",
    "    \n",
    "    # Test original version\n",
    "    start_time = time.time()\n",
    "    chunks_original = chunk_by_paragraphs(sample_text, max_chunk_size=1000, overlap_ratio=0.1)\n",
    "    original_time = time.time() - start_time\n",
    "    \n",
    "    # Test optimized version\n",
    "    start_time = time.time()\n",
    "    chunks_optimized = chunk_by_paragraphs_optimized(sample_text, max_chunk_size=1000, overlap_ratio=0.1)\n",
    "    optimized_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Original algorithm: {original_time:.4f} seconds\")\n",
    "    print(f\"‚ö° Optimized algorithm: {optimized_time:.4f} seconds\")\n",
    "    print(f\"üöÄ Speed improvement: {original_time/optimized_time:.1f}x faster\")\n",
    "    print(f\"üìà Original chunks: {len(chunks_original)}\")\n",
    "    print(f\"üìà Optimized chunks: {len(chunks_optimized)}\")\n",
    "    \n",
    "    return chunks_original, chunks_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7046af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance comparison\n",
    "print(\"Running performance comparison...\")\n",
    "original_chunks, optimized_chunks = compare_performance()\n",
    "\n",
    "# Verify both produce similar results\n",
    "print(f\"\\n‚úÖ Verification:\")\n",
    "print(f\"Both algorithms produced same number of chunks: {len(original_chunks) == len(optimized_chunks)}\")\n",
    "\n",
    "# Show sample chunks to verify overlap is working\n",
    "if len(optimized_chunks) >= 2:\n",
    "    print(f\"\\nüìù Sample overlap verification:\")\n",
    "    chunk1_end = optimized_chunks[0][-100:]  # Last 100 chars of first chunk\n",
    "    chunk2_start = optimized_chunks[1][:100]  # First 100 chars of second chunk\n",
    "    \n",
    "    # Simple overlap check\n",
    "    words1 = set(chunk1_end.split())\n",
    "    words2 = set(chunk2_start.split())\n",
    "    overlap_words = words1.intersection(words2)\n",
    "    print(f\"Common words between chunks: {len(overlap_words)} words\")\n",
    "    print(f\"Overlap detected: {len(overlap_words) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb6a91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running OPTIMIZED chunking with overlap...\n",
      "Found 89 JSON files to process\n",
      "Chunk size: 1000, Overlap: 10.0%\n",
      "\n",
      "Processing: 1) Temel kavramlar √∂nyargƒ±, kalƒ±pyargƒ± ve ayrƒ±mcƒ±lƒ±k.json... ‚úì 35 chunks ‚Üí 1) Temel kavramlar √∂nyargƒ±, kalƒ±pyargƒ± ve ayrƒ±mcƒ±lƒ±k_chunks.jsonl\n",
      "Processing: 10) T√úRKƒ∞YE‚ÄôDE √ñRG√úTLENME √ñZG√úRL√úƒû√úN√úN GENEL G√ñR√úN√úM√ú-II .json... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use the OPTIMIZED version for production processing\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This should be 3-5x faster than the original implementation\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Running OPTIMIZED chunking with overlap...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mprocess_all_json_files_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_chunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10% overlap\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Alternative: Use different overlap ratios for experimentation\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# process_all_json_files_optimized(max_chunk_size=1000, overlap_ratio=0.05)  # 5% overlap\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# process_all_json_files_optimized(max_chunk_size=1000, overlap_ratio=0.15)  # 15% overlap\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mprocess_all_json_files_optimized\u001b[39m\u001b[34m(input_dir, output_dir, max_chunk_size, overlap_ratio)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Use optimized processing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m chunks = \u001b[43mprocess_json_file_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Save as JSONL\u001b[39;00m\n\u001b[32m    211\u001b[39m output_file = output_path / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_chunks.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mprocess_json_file_optimized\u001b[39m\u001b[34m(json_path, max_chunk_size, overlap_ratio)\u001b[39m\n\u001b[32m    153\u001b[39m metadata = data.get(\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Use optimized chunking\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m chunks = \u001b[43mchunk_by_paragraphs_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Pre-allocate result list for better performance\u001b[39;00m\n\u001b[32m    159\u001b[39m result = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mchunk_by_paragraphs_optimized\u001b[39m\u001b[34m(text, max_chunk_size, overlap_ratio)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m current_chunk_paras \u001b[38;5;129;01mand\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(paragraphs):\n\u001b[32m     43\u001b[39m     oversized_para = paragraphs[i]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     sentence_chunks = \u001b[43m_split_oversized_paragraph_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43moversized_para\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     chunks.extend(sentence_chunks)\n\u001b[32m     46\u001b[39m     i += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36m_split_oversized_paragraph_optimized\u001b[39m\u001b[34m(para, max_chunk_size, overlap_size)\u001b[39m\n\u001b[32m     71\u001b[39m chunks = []\n\u001b[32m     72\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m i < \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     75\u001b[39m     current_sentences = []\n\u001b[32m     76\u001b[39m     current_length = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Use the OPTIMIZED version for production processing\n",
    "# This should be 3-5x faster than the original implementation\n",
    "\n",
    "print(\"üöÄ Running OPTIMIZED chunking with overlap...\")\n",
    "process_all_json_files_optimized(\n",
    "    max_chunk_size=1000, \n",
    "    overlap_ratio=0.1  # 10% overlap\n",
    ")\n",
    "\n",
    "# Alternative: Use different overlap ratios for experimentation\n",
    "# process_all_json_files_optimized(max_chunk_size=1000, overlap_ratio=0.05)  # 5% overlap\n",
    "# process_all_json_files_optimized(max_chunk_size=1000, overlap_ratio=0.15)  # 15% overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a033634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
