{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0d8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone\n",
      "  Using cached pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pinecone) (2025.10.5)\n",
      "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
      "  Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
      "  Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pinecone) (4.15.0)\n",
      "Collecting urllib3>=1.26.0 (from pinecone)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting requests<3.0.0,>=2.32.3 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Using cached pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
      "Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl (259 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: urllib3, pinecone-plugin-interface, packaging, requests, pinecone-plugin-assistant, pinecone\n",
      "\n",
      "   ---------------------------------------- 0/6 [urllib3]\n",
      "   ---------------------------------------- 0/6 [urllib3]\n",
      "   ------ --------------------------------- 1/6 [pinecone-plugin-interface]\n",
      "  Attempting uninstall: packaging\n",
      "   ------ --------------------------------- 1/6 [pinecone-plugin-interface]\n",
      "    Found existing installation: packaging 25.0\n",
      "   ------ --------------------------------- 1/6 [pinecone-plugin-interface]\n",
      "    Uninstalling packaging-25.0:\n",
      "   ------ --------------------------------- 1/6 [pinecone-plugin-interface]\n",
      "      Successfully uninstalled packaging-25.0\n",
      "   ------ --------------------------------- 1/6 [pinecone-plugin-interface]\n",
      "   ------------- -------------------------- 2/6 [packaging]\n",
      "   ------------- -------------------------- 2/6 [packaging]\n",
      "   -------------------- ------------------- 3/6 [requests]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 4/6 [pinecone-plugin-assistant]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   --------------------------------- ------ 5/6 [pinecone]\n",
      "   ---------------------------------------- 6/6 [pinecone]\n",
      "\n",
      "Successfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.8.0 pinecone-plugin-interface-0.0.7 requests-2.32.5 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81443ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (0.11.7)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pdfplumber->-r requirements.txt (line 1)) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pdfplumber->-r requirements.txt (line 1)) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pdfplumber->-r requirements.txt (line 1)) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber->-r requirements.txt (line 1)) (46.0.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yigit\\desktop\\enterprises\\arayuz-9\\.venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->-r requirements.txt (line 1)) (2.23)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e68fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "\n",
    "def create_ascii_id(filename: str, chunk_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Create an ASCII-safe ID using hash of filename.\n",
    "    \n",
    "    Args:\n",
    "        filename: Original filename\n",
    "        chunk_id: Chunk ID\n",
    "        \n",
    "    Returns:\n",
    "        ASCII-safe vector ID\n",
    "    \"\"\"\n",
    "    # Create a short hash of the filename\n",
    "    file_hash = hashlib.md5(filename.encode('utf-8')).hexdigest()[:12]\n",
    "    return f\"{file_hash}_{chunk_id}\"\n",
    "\n",
    "\n",
    "def create_embedding(text: str, model: str = \"text-embedding-3-large\") -> List[float]:\n",
    "    \"\"\"\n",
    "    Create embedding for a single text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        Embedding vector\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def create_embeddings_batch(texts: List[str], model: str = \"text-embedding-3-large\") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Create embeddings for multiple texts in batch.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input texts\n",
    "        model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def load_chunks_from_jsonl(jsonl_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load chunks from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_existing_vector_ids(index) -> set:\n",
    "    \"\"\"\n",
    "    Get all existing vector IDs from Pinecone index.\n",
    "    \n",
    "    Args:\n",
    "        index: Pinecone index object\n",
    "        \n",
    "    Returns:\n",
    "        Set of existing vector IDs\n",
    "    \"\"\"\n",
    "    existing_ids = set()\n",
    "    \n",
    "    # Pinecone doesn't have a direct \"list all IDs\" method\n",
    "    # We'll use a dummy query to check stats and fetch in batches\n",
    "    stats = index.describe_index_stats()\n",
    "    \n",
    "    print(f\"Index currently contains {stats.total_vector_count} vectors\")\n",
    "    \n",
    "    return existing_ids\n",
    "\n",
    "\n",
    "def check_if_file_processed(index, jsonl_file_stem: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a JSONL file has already been processed by querying for its chunks.\n",
    "    \n",
    "    Args:\n",
    "        index: Pinecone index object\n",
    "        jsonl_file_stem: Stem of the JSONL filename (without extension)\n",
    "        \n",
    "    Returns:\n",
    "        True if file has been processed, False otherwise\n",
    "    \"\"\"\n",
    "    # Try to fetch a vector with this file's prefix (using hash)\n",
    "    test_id = create_ascii_id(jsonl_file_stem, 0)\n",
    "    \n",
    "    try:\n",
    "        result = index.fetch(ids=[test_id])\n",
    "        if result.vectors:\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def process_and_upload_chunks(\n",
    "    chunks_dir: str = r\"C:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-9\\chunks\",\n",
    "    index_name: str = \"polcon\",\n",
    "    batch_size: int = 100\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process all JSONL files, create embeddings, and upload to Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        chunks_dir: Directory containing JSONL chunk files\n",
    "        index_name: Name of the Pinecone index\n",
    "        batch_size: Number of chunks to process in each batch\n",
    "    \"\"\"\n",
    "    chunks_path = Path(chunks_dir)\n",
    "    \n",
    "    # Connect to existing Pinecone index\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    # Get current index stats\n",
    "    get_existing_vector_ids(index)\n",
    "    \n",
    "    # Find all JSONL files\n",
    "    jsonl_files = list(chunks_path.glob(\"*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"No JSONL files found in {chunks_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(jsonl_files)} JSONL files to process\\n\")\n",
    "    \n",
    "    total_uploaded = 0\n",
    "    skipped_files = 0\n",
    "    \n",
    "    for jsonl_file in jsonl_files:\n",
    "        print(f\"Processing: {jsonl_file.name}\")\n",
    "        \n",
    "        # Check if this file has already been processed\n",
    "        if check_if_file_processed(index, jsonl_file.stem):\n",
    "            print(f\"  ⊘ File already processed, skipping\\n\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load chunks\n",
    "            chunks = load_chunks_from_jsonl(jsonl_file)\n",
    "            print(f\"  Loaded {len(chunks)} chunks\")\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(chunks), batch_size):\n",
    "                batch = chunks[i:i + batch_size]\n",
    "                \n",
    "                # Extract texts for embedding\n",
    "                texts = [chunk['text'] for chunk in batch]\n",
    "                \n",
    "                # Create embeddings\n",
    "                print(f\"  Creating embeddings for batch {i//batch_size + 1}...\", end=\" \")\n",
    "                embeddings = create_embeddings_batch(texts)\n",
    "                print(\"✓\")\n",
    "                \n",
    "                # Prepare vectors for Pinecone\n",
    "                vectors = []\n",
    "                for j, (chunk, embedding) in enumerate(zip(batch, embeddings)):\n",
    "                    vector_id = create_ascii_id(jsonl_file.stem, chunk['chunk_id'])\n",
    "                    \n",
    "                    metadata = {\n",
    "                        \"text\": chunk['text'],\n",
    "                        \"source_file\": chunk['source_file'],\n",
    "                        \"source_path\": chunk['source_path'],\n",
    "                        \"chunk_id\": chunk['chunk_id'],\n",
    "                        \"char_count\": chunk['char_count'],\n",
    "                        \"word_count\": chunk['word_count'],\n",
    "                        \"jsonl_file\": jsonl_file.stem\n",
    "                    }\n",
    "                    \n",
    "                    vectors.append({\n",
    "                        \"id\": vector_id,\n",
    "                        \"values\": embedding,\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "                \n",
    "                # Upload to Pinecone\n",
    "                print(f\"  Uploading batch to Pinecone...\", end=\" \")\n",
    "                index.upsert(vectors=vectors)\n",
    "                print(\"✓\")\n",
    "                \n",
    "                total_uploaded += len(vectors)\n",
    "                \n",
    "                # Small delay to avoid rate limits\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            print(f\"  ✓ Completed {jsonl_file.name}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing {jsonl_file.name}: {e}\\n\")\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Files skipped (already processed): {skipped_files}\")\n",
    "    print(f\"Total new vectors uploaded: {total_uploaded}\")\n",
    "    \n",
    "    # Get index stats\n",
    "    stats = index.describe_index_stats()\n",
    "    print(f\"Total vectors in index: {stats.total_vector_count}\")\n",
    "\n",
    "\n",
    "def query_similar_chunks(\n",
    "    query: str,\n",
    "    index_name: str = \"polcon\",\n",
    "    top_k: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Query Pinecone for similar chunks.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        index_name: Name of the Pinecone index\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of similar chunks with scores\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embedding(query)\n",
    "    \n",
    "    # Query Pinecone\n",
    "    index = pc.Index(index_name)\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    matches = []\n",
    "    for match in results.matches:\n",
    "        matches.append({\n",
    "            \"score\": match.score,\n",
    "            \"text\": match.metadata.get('text', ''),\n",
    "            \"source_file\": match.metadata.get('source_file', ''),\n",
    "            \"chunk_id\": match.metadata.get('chunk_id', '')\n",
    "        })\n",
    "    \n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb5965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index currently contains 0 vectors\n",
      "Found 4 JSONL files to process\n",
      "\n",
      "Processing: 1) Temel kavramlar önyargı, kalıpyargı ve ayrımcılık_chunks.jsonl\n",
      "  Loaded 34 chunks\n",
      "  Creating embeddings for batch 1... ✓\n",
      "  Uploading batch to Pinecone... ✓\n",
      "  ✓ Completed 1) Temel kavramlar önyargı, kalıpyargı ve ayrımcılık_chunks.jsonl\n",
      "\n",
      "Processing: 2) Ayrımcılık ve medya_chunks.jsonl\n",
      "  Loaded 45 chunks\n",
      "  Creating embeddings for batch 1... ✓\n",
      "  Uploading batch to Pinecone... ✓\n",
      "  ✓ Completed 2) Ayrımcılık ve medya_chunks.jsonl\n",
      "\n",
      "Processing: 3) Toplumsal Cinsiyete Dayalı Ayrımcılık_chunks.jsonl\n",
      "  Loaded 33 chunks\n",
      "  Creating embeddings for batch 1... ✓\n",
      "  Uploading batch to Pinecone... ✓\n",
      "  ✓ Completed 3) Toplumsal Cinsiyete Dayalı Ayrımcılık_chunks.jsonl\n",
      "\n",
      "Processing: 4) Uluslararası Af Örgütü Raporu 2021-2022 Avrupa ve Orta Asya Değerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "  Loaded 35 chunks\n",
      "  Creating embeddings for batch 1... ✓\n",
      "  Uploading batch to Pinecone... ✓\n",
      "  ✓ Completed 4) Uluslararası Af Örgütü Raporu 2021-2022 Avrupa ve Orta Asya Değerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "\n",
      "\n",
      "Processing complete!\n",
      "Files skipped (already processed): 0\n",
      "Total new vectors uploaded: 147\n",
      "Total vectors in index: 147\n"
     ]
    }
   ],
   "source": [
    "# Run the uploader\n",
    "if __name__ == \"__main__\":\n",
    "    # Upload all chunks to Pinecone\n",
    "    process_and_upload_chunks()\n",
    "    \n",
    "    # Example query\n",
    "    # results = query_similar_chunks(\"gençlik örgütleri nedir?\")\n",
    "    # for i, result in enumerate(results):\n",
    "    #     print(f\"\\n{i+1}. Score: {result['score']:.4f}\")\n",
    "    #     print(f\"Source: {result['source_file']}\")\n",
    "    #     print(f\"Text: {result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9224444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
