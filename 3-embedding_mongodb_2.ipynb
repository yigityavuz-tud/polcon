{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Set\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Configuration\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\", \"mongodb+srv://username:password@polcon.mongodb.net/\")\n",
    "DATABASE_NAME = \"polcon_rag\"\n",
    "COLLECTION_NAME = \"document_chunks\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"  # 3072 dimensions\n",
    "BATCH_SIZE = 100  # Process embeddings in batches\n",
    "CHUNKS_FOLDER = \"./chunks\"  # Path to chunks folder\n",
    "\n",
    "def get_existing_chunk_ids(collection) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Retrieve all existing chunk IDs from MongoDB.\n",
    "    \"\"\"\n",
    "    print(\"üîç Checking for existing chunks in MongoDB...\")\n",
    "    existing_ids = set()\n",
    "    \n",
    "    # Only fetch the chunk_id field to minimize data transfer\n",
    "    cursor = collection.find({}, {'chunk_id': 1, '_id': 0})\n",
    "    \n",
    "    for doc in cursor:\n",
    "        if 'chunk_id' in doc:\n",
    "            existing_ids.add(doc['chunk_id'])\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(existing_ids)} existing chunks in database\")\n",
    "    return existing_ids\n",
    "\n",
    "def find_jsonl_files(folder_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all JSONL files in the specified folder.\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    if not folder.exists():\n",
    "        print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all .jsonl files\n",
    "    jsonl_files = list(folder.glob(\"*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"‚ö†Ô∏è  No JSONL files found in: {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nüìÅ Found {len(jsonl_files)} JSONL file(s) in {folder_path}:\")\n",
    "    for file in jsonl_files:\n",
    "        print(f\"   - {file.name}\")\n",
    "    \n",
    "    return [str(f) for f in jsonl_files]\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load chunks from a single JSONL file.\"\"\"\n",
    "    chunks = []\n",
    "    file_name = Path(file_path).name\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    chunk = json.loads(line)\n",
    "                    # Validate required fields\n",
    "                    if 'chunk_id' not in chunk or 'text' not in chunk:\n",
    "                        print(f\"‚ö†Ô∏è  {file_name} line {line_num}: Missing chunk_id or text\")\n",
    "                        continue\n",
    "                    chunks.append(chunk)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  {file_name} line {line_num}: Invalid JSON - {e}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_all_jsonl_files(folder_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load chunks from all JSONL files in the folder.\n",
    "    \"\"\"\n",
    "    jsonl_files = find_jsonl_files(folder_path)\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        return []\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    print(f\"\\nüìÇ Loading chunks from {len(jsonl_files)} file(s)...\")\n",
    "    for file_path in jsonl_files:\n",
    "        file_name = Path(file_path).name\n",
    "        chunks = load_jsonl(file_path)\n",
    "        print(f\"   ‚úÖ {file_name}: {len(chunks)} chunks\")\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def get_embeddings(texts: List[str], client: OpenAI) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a batch of texts using OpenAI.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "def embed_and_insert_chunks(chunks_folder: str = CHUNKS_FOLDER, force_reindex: bool = False):\n",
    "    \"\"\"\n",
    "    Main function to embed chunks from all JSONL files and insert into MongoDB.\n",
    "    Only processes new chunks that haven't been inserted before.\n",
    "    \n",
    "    Args:\n",
    "        chunks_folder: Path to the folder containing JSONL files\n",
    "        force_reindex: If True, re-embed and update all chunks even if they exist\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize clients\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    mongo_client = MongoClient(MONGODB_URI)\n",
    "    db = mongo_client[DATABASE_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    \n",
    "    # Create a unique index on chunk_id to prevent duplicates\n",
    "    collection.create_index(\"chunk_id\", unique=True)\n",
    "    print(f\"‚úÖ Connected to MongoDB Atlas\")\n",
    "    print(f\"   Project: Arayuz\")\n",
    "    print(f\"   Cluster: polcon\")\n",
    "    print(f\"   Database: {DATABASE_NAME}\")\n",
    "    print(f\"   Collection: {COLLECTION_NAME}\")\n",
    "    \n",
    "    # Load chunks from all JSONL files in the folder\n",
    "    chunks = load_all_jsonl_files(chunks_folder)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"\\n‚ùå No chunks loaded. Please check the folder path and file contents.\")\n",
    "        mongo_client.close()\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total chunks loaded: {len(chunks)}\")\n",
    "    \n",
    "    # Get existing chunk IDs from database\n",
    "    if not force_reindex:\n",
    "        existing_ids = get_existing_chunk_ids(collection)\n",
    "        \n",
    "        # Filter out chunks that already exist\n",
    "        new_chunks = [chunk for chunk in chunks if chunk['chunk_id'] not in existing_ids]\n",
    "        \n",
    "        print(f\"\\nüìä Deduplication Status:\")\n",
    "        print(f\"   Total chunks in files: {len(chunks)}\")\n",
    "        print(f\"   Already in database: {len(chunks) - len(new_chunks)}\")\n",
    "        print(f\"   New chunks to process: {len(new_chunks)}\")\n",
    "        \n",
    "        if len(new_chunks) == 0:\n",
    "            print(\"\\n‚úÖ All chunks already exist in database. Nothing to do!\")\n",
    "            print(\"üí° Use force_reindex=True to re-embed existing chunks\")\n",
    "            mongo_client.close()\n",
    "            return\n",
    "        \n",
    "        chunks_to_process = new_chunks\n",
    "    else:\n",
    "        print(f\"\\nüîÑ Force reindex enabled - processing all {len(chunks)} chunks\")\n",
    "        chunks_to_process = chunks\n",
    "    \n",
    "    # Check for duplicate chunk_ids in the loaded data\n",
    "    chunk_ids = [chunk['chunk_id'] for chunk in chunks_to_process]\n",
    "    duplicate_ids = set([x for x in chunk_ids if chunk_ids.count(x) > 1])\n",
    "    if duplicate_ids:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Found {len(duplicate_ids)} duplicate chunk_ids in source files\")\n",
    "        print(f\"   First few: {list(duplicate_ids)[:5]}\")\n",
    "    \n",
    "    # Process chunks in batches\n",
    "    documents_to_insert = []\n",
    "    \n",
    "    print(f\"\\nü§ñ Generating embeddings for {len(chunks_to_process)} chunks...\")\n",
    "    print(f\"   Using model: {EMBEDDING_MODEL}\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks_to_process), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch = chunks_to_process[i:i + BATCH_SIZE]\n",
    "        \n",
    "        # Extract text from each chunk\n",
    "        texts = [chunk['text'] for chunk in batch]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        try:\n",
    "            embeddings = get_embeddings(texts, openai_client)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error generating embeddings for batch {i//BATCH_SIZE + 1}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare documents for insertion\n",
    "        for chunk, embedding in zip(batch, embeddings):\n",
    "            doc = {\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'text': chunk['text'],\n",
    "                'embedding': embedding,\n",
    "                'char_count': chunk.get('char_count', len(chunk['text'])),\n",
    "                'word_count': chunk.get('word_count', len(chunk['text'].split())),\n",
    "                'source_file': chunk.get('source_file', 'unknown')\n",
    "            }\n",
    "            documents_to_insert.append(doc)\n",
    "    \n",
    "    # Insert new documents\n",
    "    print(f\"\\nüíæ Inserting {len(documents_to_insert)} documents into MongoDB...\")\n",
    "    try:\n",
    "        if force_reindex:\n",
    "            # Use bulk write with upsert for force reindex\n",
    "            from pymongo import UpdateOne\n",
    "            operations = [\n",
    "                UpdateOne(\n",
    "                    {'chunk_id': doc['chunk_id']},\n",
    "                    {'$set': doc},\n",
    "                    upsert=True\n",
    "                )\n",
    "                for doc in documents_to_insert\n",
    "            ]\n",
    "            result = collection.bulk_write(operations)\n",
    "            print(f\"‚úÖ Upsert complete:\")\n",
    "            print(f\"   Inserted: {result.upserted_count}\")\n",
    "            print(f\"   Modified: {result.modified_count}\")\n",
    "        else:\n",
    "            # Regular insert for new documents\n",
    "            result = collection.insert_many(documents_to_insert, ordered=False)\n",
    "            print(f\"‚úÖ Successfully inserted {len(result.inserted_ids)} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Insertion completed with some warnings: {e}\")\n",
    "    \n",
    "    mongo_client.close()\n",
    "    print(\"\\n‚úÖ Embedding and insertion complete!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã NEXT STEP: Create Vector Search Index in Atlas UI\")\n",
    "    print(\"=\"*70)\n",
    "    print_index_instructions()\n",
    "\n",
    "def print_index_instructions():\n",
    "    \"\"\"Print instructions for creating the vector search index.\"\"\"\n",
    "    \n",
    "    index_definition = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 3072,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filter\",\n",
    "                \"path\": \"source_file\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filter\",\n",
    "                \"path\": \"chunk_id\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîß CREATE VECTOR SEARCH INDEX:\")\n",
    "    print(\"\\n1. Go to MongoDB Atlas: https://cloud.mongodb.com\")\n",
    "    print(\"2. Navigate to your 'polcon' cluster\")\n",
    "    print(\"3. Click on 'Atlas Search' tab\")\n",
    "    print(\"4. Click 'Create Search Index'\")\n",
    "    print(\"5. Choose 'Atlas Vector Search' ‚Üí 'JSON Editor'\")\n",
    "    print(\"6. Configure:\")\n",
    "    print(f\"   - Database: {DATABASE_NAME}\")\n",
    "    print(f\"   - Collection: {COLLECTION_NAME}\")\n",
    "    print(\"   - Index Name: vector_index\")\n",
    "    print(\"\\n7. Paste this JSON definition:\\n\")\n",
    "    print(json.dumps(index_definition, indent=2))\n",
    "    print(\"\\n8. Click 'Create Search Index'\")\n",
    "    print(\"\\n‚è±Ô∏è  Index creation takes 5-10 minutes. You'll get a notification when ready.\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "def query_vector_store(query_text: str, top_k: int = 5, source_file: str = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Query the vector store with a text query.\n",
    "    Use this after creating the vector search index in Atlas.\n",
    "    \n",
    "    Args:\n",
    "        query_text: The search query\n",
    "        top_k: Number of results to return\n",
    "        source_file: Optional filter by source file\n",
    "    \"\"\"\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    mongo_client = MongoClient(MONGODB_URI)\n",
    "    db = mongo_client[DATABASE_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    \n",
    "    print(f\"üîç Searching for: '{query_text}'\")\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = get_embeddings([query_text], openai_client)[0]\n",
    "    \n",
    "    # Build vector search pipeline\n",
    "    vector_search_stage = {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": top_k * 10,\n",
    "            \"limit\": top_k\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add source file filter if provided\n",
    "    if source_file:\n",
    "        vector_search_stage[\"$vectorSearch\"][\"filter\"] = {\n",
    "            \"source_file\": {\"$eq\": source_file}\n",
    "        }\n",
    "        print(f\"üìÅ Filtering by source: {source_file}\")\n",
    "    \n",
    "    pipeline = [\n",
    "        vector_search_stage,\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"chunk_id\": 1,\n",
    "                \"text\": 1,\n",
    "                \"source_file\": 1,\n",
    "                \"char_count\": 1,\n",
    "                \"word_count\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        mongo_client.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        mongo_client.close()\n",
    "        print(f\"‚ùå Error querying vector store: {e}\")\n",
    "        print(\"üí° Make sure you've created the 'vector_index' in Atlas UI\")\n",
    "        return []\n",
    "\n",
    "def get_collection_stats():\n",
    "    \"\"\"Get statistics about the vector store.\"\"\"\n",
    "    mongo_client = MongoClient(MONGODB_URI)\n",
    "    db = mongo_client[DATABASE_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    \n",
    "    total_chunks = collection.count_documents({})\n",
    "    \n",
    "    # Get unique source files\n",
    "    pipeline = [\n",
    "        {\"$group\": {\"_id\": \"$source_file\"}},\n",
    "        {\"$sort\": {\"_id\": 1}}\n",
    "    ]\n",
    "    unique_sources = list(collection.aggregate(pipeline))\n",
    "    \n",
    "    # Get total word count\n",
    "    pipeline = [\n",
    "        {\"$group\": {\"_id\": None, \"total_words\": {\"$sum\": \"$word_count\"}}}\n",
    "    ]\n",
    "    total_words_result = list(collection.aggregate(pipeline))\n",
    "    total_words = total_words_result[0]['total_words'] if total_words_result else 0\n",
    "    \n",
    "    print(f\"\\nüìä Vector Store Statistics:\")\n",
    "    print(f\"   Total chunks: {total_chunks:,}\")\n",
    "    print(f\"   Total words: {total_words:,}\")\n",
    "    print(f\"   Unique source files: {len(unique_sources)}\")\n",
    "    if unique_sources and len(unique_sources) <= 10:\n",
    "        print(f\"\\n   Source files:\")\n",
    "        for source in unique_sources:\n",
    "            count = collection.count_documents({\"source_file\": source['_id']})\n",
    "            print(f\"      - {source['_id']}: {count} chunks\")\n",
    "    \n",
    "    mongo_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9224444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to MongoDB Atlas\n",
      "   Project: Arayuz\n",
      "   Cluster: polcon\n",
      "   Database: polcon_rag\n",
      "   Collection: document_chunks\n",
      "\n",
      "üìÅ Found 89 JSONL file(s) in ./chunks:\n",
      "   - 1) Temel kavramlar √∂nyargƒ±, kalƒ±pyargƒ± ve ayrƒ±mcƒ±lƒ±k_chunks.jsonl\n",
      "   - 10) T√úRKƒ∞YE‚ÄôDE √ñRG√úTLENME √ñZG√úRL√úƒû√úN√úN GENEL G√ñR√úN√úM√ú-II _chunks.jsonl\n",
      "   - 11) Yurttaslik_Alani_Bilgi_Notu_1_chunks.jsonl\n",
      "   - 12) TER√ñRLE M√úCADELEYƒ∞ ARA√áSALLA≈ûTIRMAK_chunks.jsonl\n",
      "   - 13) PROTESTO HAKKINI KORU_chunks.jsonl\n",
      "   - 14) KomploTeorileri_AR_23.03.23_web_chunks.jsonl\n",
      "   - 15) Feminist_Hareketin_Gundemleri__chunks.jsonl\n",
      "   - 16) Sivil Toplum Kurulu≈ülarƒ±nƒ±n Devlet Tarafƒ±ndan Finansmanƒ± √úzerine Bir Tartƒ±≈üma_chunks.jsonl\n",
      "   - 17) Gen√ßlik Politikalarƒ±nda Kar≈üƒ±la≈ütƒ±rmalƒ± Bir Deƒüerlendirme-T√ºrkiye ve Finlandiya √ñrneƒüi_chunks.jsonl\n",
      "   - 18) Avrupa Konseyi Politik Karar Alma S√ºre√ßlerine Sivil Katƒ±lƒ±m Rehberi √áevirisi_chunks.jsonl\n",
      "   - 19) Kamp√ºsten √ñƒürenci Topluluklarƒ± _chunks.jsonl\n",
      "   - 2) Ayrƒ±mcƒ±lƒ±k ve medya_chunks.jsonl\n",
      "   - 20) Gen√ßler Ne(ler) ƒ∞stiyor_ _chunks.jsonl\n",
      "   - 21) T√ºrkiye‚Äôde Gen√ßlik ve Siyaset_ Gelecek ƒ∞√ßin Nasƒ±l Bir Katƒ±lƒ±m_ _chunks.jsonl\n",
      "   - 22) Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 13.sayƒ±_chunks.jsonl\n",
      "   - 23) T√ºrkiye_de Gen√ßlik Miti 1980 Sonrasƒ± T√ºrkiye Gen√ßliƒüi ƒ∞leti≈üim Yayƒ±nlarƒ±_chunks.jsonl\n",
      "   - 24) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- _chunks.jsonl\n",
      "   - 25) T√ºrkiye‚Äôde Gen√ßlerin G√ºvencesizliƒüi_ √áalƒ±≈üma, Ge√ßim ve Ya≈üam Algƒ±sƒ±_chunks.jsonl\n",
      "   - 26) Toplumun Boƒüazi√ßi √úniversitesi Olaylarƒ±na Bakƒ±≈üƒ±_chunks.jsonl\n",
      "   - 27) K√ºrt Gen√ßler‚Äô20 Benzerlikler Farklar Deƒüi≈üimler_chunks.jsonl\n",
      "   - 28) NEET Gen√ßler Ara≈ütƒ±rmasƒ± ‚Äì NEET Gen√ßlerin ƒ∞nsan Onuruna Yara≈üƒ±r Ya≈üam S√ºrme Hakkƒ±na Eri≈üimi_chunks.jsonl\n",
      "   - 29) TGSP T√ºrkiye‚Äônin Gen√ßleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "   - 3) Toplumsal Cinsiyete Dayalƒ± Ayrƒ±mcƒ±lƒ±k_chunks.jsonl\n",
      "   - 30) TOG Gen√ßlik √áalƒ±≈ümasƒ±nƒ±n Toplumsal Katƒ±lƒ±ma Etkisi Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "   - 31) T√ºrkiye‚Äôde Gen√ßlerin ƒ∞yi Olma Hali Saha Ara≈ütƒ±rmasƒ± Bulgularƒ±- HABƒ∞TAT- _chunks.jsonl\n",
      "   - 32) T√ºrkiye Gen√ßlik Ara≈ütƒ±rmasƒ± 2021_chunks.jsonl\n",
      "   - 33) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- 2020_chunks.jsonl\n",
      "   - 34)Uluslararasƒ± Af √ñrg√ºt√º_chunks.jsonl\n",
      "   - 37) Perspectives on Youth Participation_chunks.jsonl\n",
      "   - 38) Young people‚Äôs right to assemble peacefully_chunks.jsonl\n",
      "   - 39) Shrinking democratic civic space for youth_chunks.jsonl\n",
      "   - 4) Uluslararasƒ± Af √ñrg√ºt√º Raporu 2021-2022 Avrupa ve Orta Asya Deƒüerlendirmesi(sayfa 46-54)_chunks.jsonl\n",
      "   - 40) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ±_chunks.jsonl\n",
      "   - 41) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ± Politika √ñnerileri_chunks.jsonl\n",
      "   - 42) T√ºrkiye_de ifade ve medya √∂zg√ºrl√ºƒü√º ve insan haklarƒ± savunucularƒ± ile sivil toplumun durumu hakkƒ±ndaki memorandum_chunks.jsonl\n",
      "   - 43) T√ºrkiye‚Äôdeki Gen√ßlik √ñrg√ºtlerinin ƒ∞htiya√ß Analizi Raporu 2025_chunks.jsonl\n",
      "   - 44)Toplumsal Deƒüerler ve Gen√ßlik Ara≈ütƒ±rma Raporu_chunks.jsonl\n",
      "   - 45) Gen√ßlerin Politik Tercihleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "   - 46)Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 35.sayƒ±_chunks.jsonl\n",
      "   - 47)Gen√ßler i√ßin nasƒ±l bir kent__chunks.jsonl\n",
      "   - 48) TGSP Gen√ßlerin G√∂n√ºll√ºl√ºk Algƒ±sƒ±_chunks.jsonl\n",
      "   - 49) Yerel Yonetimlere Iliskin PolitikaBelgesi_chunks.jsonl\n",
      "   - 5) Paralel Kariyer Arayƒ±≈üƒ±nƒ±n Nedenleri Isparta‚Äôda Faaliyet G√∂steren STK‚Äôlarda Bir Ara≈ütƒ±rma_chunks.jsonl\n",
      "   - 50) Milliyet√ßiliƒüin D√∂n√º≈ü√ºm√º ve Gen√ß Y√ºzleri_chunks.jsonl\n",
      "   - 51)Gen√ßlik Ara≈ütƒ±rmasƒ± - T√ºrkiye 2024_chunks.jsonl\n",
      "   - 52)CORE Gen√ßlerin Se√ßimi_chunks.jsonl\n",
      "   - 53) Gen√ßlerin G√º√ßlendirilmesine Y√∂nelik Harcamalarƒ± ƒ∞zleme Kƒ±lavuzu_chunks.jsonl\n",
      "   - 54) Gen√ßlerin Siyasi Katƒ±lƒ±mƒ± - ≈ûebeke 1_chunks.jsonl\n",
      "   - 55) T√ºrkiye_de Gen√ßlerin Katƒ±lƒ±mƒ± - ≈ûebeke_chunks.jsonl\n",
      "   - 56) COVID-19 Pandemisi S√ºrecinde Gen√ßlerin ƒ∞yilik Halinin Belirlenmesi Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "   - 57) KONDA - Hafƒ±za Merkezi GencÃßlerin IÃánsan Haklarƒ± Algƒ±sƒ±__chunks.jsonl\n",
      "   - 58) T√ºrkiye_de Gen√ßlik √áalƒ±≈ümasƒ± ve Politikasƒ±_chunks.jsonl\n",
      "   - 59) LGBTIÃá gencÃßler gencÃßlik merkezlerinde ne istiyor__chunks.jsonl\n",
      "   - 6) Sivil Toplum √ñrg√ºtlerinde Profesyonel ve G√∂n√ºll√º √áalƒ±≈üma ƒ∞li≈ükileri Tehditler Ve Fƒ±rsatlar_chunks.jsonl\n",
      "   - 60) GencÃß Kadƒ±nlarƒ±n Karar Alma Mekanizmalarƒ±na Katƒ±lƒ±mƒ±_chunks.jsonl\n",
      "   - 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi_chunks.jsonl\n",
      "   - 62) TGSP TuÃàrkiye_nin GencÃßleri Yurtdƒ±sÃßƒ± Algƒ±sƒ±_chunks.jsonl\n",
      "   - 63) TGSP TuÃàrkiye_nin GencÃßleri Dindarlƒ±k Algƒ±sƒ±_chunks.jsonl\n",
      "   - 64) TGSP TuÃàrkiye_nin GencÃßleri YuÃàksekoÃàgÃÜrenim Algƒ±sƒ±_chunks.jsonl\n",
      "   - 65 ) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± 2023_chunks.jsonl\n",
      "   - 66) SODEV Genclik Arastirmasi Raporu 2021_chunks.jsonl\n",
      "   - 67) SERHAT TRA2 GencÃßlik ArasÃßtƒ±rmasƒ±_chunks.jsonl\n",
      "   - 68) ƒ∞PA ƒ∞stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim_chunks.jsonl\n",
      "   - 69) ƒ∞PA √úniversite Mezunu Ev Gen√ßleri Ara≈ütƒ±rmasƒ±_chunks.jsonl\n",
      "   - 7) e≈üitsiz demokrasiler_chunks.jsonl\n",
      "   - 70) IPM TuÃàrkiye_de GencÃßlerin Yurtdƒ±sÃßƒ±nda YasÃßama IÃástegÃÜi_chunks.jsonl\n",
      "   - 71) IÃáPM TuÃàrkiye_de Akƒ±llƒ± KentlesÃßme ve GencÃßlik Politikalarƒ±_chunks.jsonl\n",
      "   - 72) TUÃàSES GencÃßlerin Cinsel SagÃÜlƒ±k ve UÃàreme Saƒülƒ±ƒüƒ± ArasÃßtƒ±rmasƒ±_chunks.jsonl\n",
      "   - 73) FES Youth Study Southeast Europe _chunks.jsonl\n",
      "   - 74) Biarada IÃástanbul‚Äôda GencÃßlik, Kent YurttasÃßlƒ±gÃÜƒ± ve Yerel YoÃànetim_chunks.jsonl\n",
      "   - 75) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± OÃàzet Bulgular 2023_chunks.jsonl\n",
      "   - 76) Toplum CÃßalƒ±sÃßmasƒ± EnstituÃàsuÃà Kim Bu GencÃßler__chunks.jsonl\n",
      "   - 77) KAOS GL LGBTIÃá+ OÃàgÃÜrenciler_chunks.jsonl\n",
      "   - 78) TOG UÃàniversiteli GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2024_chunks.jsonl\n",
      "   - 79) Haberlerdeki √úniversite 2022_chunks.jsonl\n",
      "   - 8) genc-oy-strateji-rapor_chunks.jsonl\n",
      "   - 80) TOG GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2022_chunks.jsonl\n",
      "   - 81) Yereliz GENCÃßLIÃáK ALANINDA CÃßALISÃßAN SIÃáVIÃáL TOPLUM OÃàRGUÃàTLERIÃá IÃáCÃßIÃáN YEREL SAVUNUCULUK REHBERIÃá_chunks.jsonl\n",
      "   - 82) KONDA Barometre 2024_chunks.jsonl\n",
      "   - 83) OECD Youth Policy Toolkit_chunks.jsonl\n",
      "   - 84) TIÃáP_li OÃàgÃÜrenciler Barƒ±nma Raporu 2023_chunks.jsonl\n",
      "   - 85) ILO Global Employement Trends for Youth 2020_chunks.jsonl\n",
      "   - 86) GoFor Hangi Gen√ß__chunks.jsonl\n",
      "   - 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu_chunks.jsonl\n",
      "   - 88) TuÃàrkiye_de NEET UÃàzerine Yapƒ±lmƒ±sÃß √áalƒ±≈ümalara ƒ∞li≈ükin Bir DegÃÜerlendirme 2024_chunks.jsonl\n",
      "   - 89) UNFPA IÃástatisliklerle GencÃßlik_chunks.jsonl\n",
      "   - 9) Politik Karar Verme S√ºre√ßlerine Etkili ve Anlamlƒ± KATILIM HAKKI ve MEKANƒ∞ZMALAR_chunks.jsonl\n",
      "   - 90) FES Genclerin GoÃàzuÃànden Dindar-SekuÃàler Eksenli KutuplasÃßma_chunks.jsonl\n",
      "   - 91) Veriler_chunks.jsonl\n",
      "\n",
      "üìÇ Loading chunks from 89 file(s)...\n",
      "   ‚úÖ 1) Temel kavramlar √∂nyargƒ±, kalƒ±pyargƒ± ve ayrƒ±mcƒ±lƒ±k_chunks.jsonl: 183 chunks\n",
      "   ‚úÖ 10) T√úRKƒ∞YE‚ÄôDE √ñRG√úTLENME √ñZG√úRL√úƒû√úN√úN GENEL G√ñR√úN√úM√ú-II _chunks.jsonl: 1162 chunks\n",
      "   ‚úÖ 11) Yurttaslik_Alani_Bilgi_Notu_1_chunks.jsonl: 196 chunks\n",
      "   ‚úÖ 12) TER√ñRLE M√úCADELEYƒ∞ ARA√áSALLA≈ûTIRMAK_chunks.jsonl: 457 chunks\n",
      "   ‚úÖ 13) PROTESTO HAKKINI KORU_chunks.jsonl: 662 chunks\n",
      "   ‚úÖ 14) KomploTeorileri_AR_23.03.23_web_chunks.jsonl: 716 chunks\n",
      "   ‚úÖ 15) Feminist_Hareketin_Gundemleri__chunks.jsonl: 275 chunks\n",
      "   ‚úÖ 16) Sivil Toplum Kurulu≈ülarƒ±nƒ±n Devlet Tarafƒ±ndan Finansmanƒ± √úzerine Bir Tartƒ±≈üma_chunks.jsonl: 269 chunks\n",
      "   ‚úÖ 17) Gen√ßlik Politikalarƒ±nda Kar≈üƒ±la≈ütƒ±rmalƒ± Bir Deƒüerlendirme-T√ºrkiye ve Finlandiya √ñrneƒüi_chunks.jsonl: 495 chunks\n",
      "   ‚úÖ 18) Avrupa Konseyi Politik Karar Alma S√ºre√ßlerine Sivil Katƒ±lƒ±m Rehberi √áevirisi_chunks.jsonl: 181 chunks\n",
      "   ‚úÖ 19) Kamp√ºsten √ñƒürenci Topluluklarƒ± _chunks.jsonl: 2324 chunks\n",
      "   ‚úÖ 2) Ayrƒ±mcƒ±lƒ±k ve medya_chunks.jsonl: 215 chunks\n",
      "   ‚úÖ 20) Gen√ßler Ne(ler) ƒ∞stiyor_ _chunks.jsonl: 252 chunks\n",
      "   ‚úÖ 21) T√ºrkiye‚Äôde Gen√ßlik ve Siyaset_ Gelecek ƒ∞√ßin Nasƒ±l Bir Katƒ±lƒ±m_ _chunks.jsonl: 630 chunks\n",
      "   ‚úÖ 22) Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 13.sayƒ±_chunks.jsonl: 3842 chunks\n",
      "   ‚úÖ 23) T√ºrkiye_de Gen√ßlik Miti 1980 Sonrasƒ± T√ºrkiye Gen√ßliƒüi ƒ∞leti≈üim Yayƒ±nlarƒ±_chunks.jsonl: 3628 chunks\n",
      "   ‚úÖ 24) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- _chunks.jsonl: 75 chunks\n",
      "   ‚úÖ 25) T√ºrkiye‚Äôde Gen√ßlerin G√ºvencesizliƒüi_ √áalƒ±≈üma, Ge√ßim ve Ya≈üam Algƒ±sƒ±_chunks.jsonl: 1073 chunks\n",
      "   ‚úÖ 26) Toplumun Boƒüazi√ßi √úniversitesi Olaylarƒ±na Bakƒ±≈üƒ±_chunks.jsonl: 225 chunks\n",
      "   ‚úÖ 27) K√ºrt Gen√ßler‚Äô20 Benzerlikler Farklar Deƒüi≈üimler_chunks.jsonl: 816 chunks\n",
      "   ‚úÖ 28) NEET Gen√ßler Ara≈ütƒ±rmasƒ± ‚Äì NEET Gen√ßlerin ƒ∞nsan Onuruna Yara≈üƒ±r Ya≈üam S√ºrme Hakkƒ±na Eri≈üimi_chunks.jsonl: 1197 chunks\n",
      "   ‚úÖ 29) TGSP T√ºrkiye‚Äônin Gen√ßleri Ara≈ütƒ±rmasƒ±_chunks.jsonl: 396 chunks\n",
      "   ‚úÖ 3) Toplumsal Cinsiyete Dayalƒ± Ayrƒ±mcƒ±lƒ±k_chunks.jsonl: 201 chunks\n",
      "   ‚úÖ 30) TOG Gen√ßlik √áalƒ±≈ümasƒ±nƒ±n Toplumsal Katƒ±lƒ±ma Etkisi Ara≈ütƒ±rmasƒ±_chunks.jsonl: 1329 chunks\n",
      "   ‚úÖ 31) T√ºrkiye‚Äôde Gen√ßlerin ƒ∞yi Olma Hali Saha Ara≈ütƒ±rmasƒ± Bulgularƒ±- HABƒ∞TAT- _chunks.jsonl: 1688 chunks\n",
      "   ‚úÖ 32) T√ºrkiye Gen√ßlik Ara≈ütƒ±rmasƒ± 2021_chunks.jsonl: 8311 chunks\n",
      "   ‚úÖ 33) T√ºrkiye‚Äônin Gen√ßliƒüi Ara≈ütƒ±rmasƒ± Raporu -SODEV- 2020_chunks.jsonl: 75 chunks\n",
      "   ‚úÖ 34)Uluslararasƒ± Af √ñrg√ºt√º_chunks.jsonl: 1400 chunks\n",
      "   ‚úÖ 37) Perspectives on Youth Participation_chunks.jsonl: 380 chunks\n",
      "   ‚úÖ 38) Young people‚Äôs right to assemble peacefully_chunks.jsonl: 538 chunks\n",
      "   ‚úÖ 39) Shrinking democratic civic space for youth_chunks.jsonl: 490 chunks\n",
      "   ‚úÖ 4) Uluslararasƒ± Af √ñrg√ºt√º Raporu 2021-2022 Avrupa ve Orta Asya Deƒüerlendirmesi(sayfa 46-54)_chunks.jsonl: 220 chunks\n",
      "   ‚úÖ 40) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ±_chunks.jsonl: 1393 chunks\n",
      "   ‚úÖ 41) T√ºrkiye‚Äôde Gen√ß ƒ∞ntiharlarƒ± Politika √ñnerileri_chunks.jsonl: 43 chunks\n",
      "   ‚úÖ 42) T√ºrkiye_de ifade ve medya √∂zg√ºrl√ºƒü√º ve insan haklarƒ± savunucularƒ± ile sivil toplumun durumu hakkƒ±ndaki memorandum_chunks.jsonl: 419 chunks\n",
      "   ‚úÖ 43) T√ºrkiye‚Äôdeki Gen√ßlik √ñrg√ºtlerinin ƒ∞htiya√ß Analizi Raporu 2025_chunks.jsonl: 999 chunks\n",
      "   ‚úÖ 44)Toplumsal Deƒüerler ve Gen√ßlik Ara≈ütƒ±rma Raporu_chunks.jsonl: 325 chunks\n",
      "   ‚úÖ 45) Gen√ßlerin Politik Tercihleri Ara≈ütƒ±rmasƒ±_chunks.jsonl: 836 chunks\n",
      "   ‚úÖ 46)Gen√ßlik Ara≈ütƒ±rmalarƒ± Dergisi 35.sayƒ±_chunks.jsonl: 3609 chunks\n",
      "   ‚úÖ 47)Gen√ßler i√ßin nasƒ±l bir kent__chunks.jsonl: 180 chunks\n",
      "   ‚úÖ 48) TGSP Gen√ßlerin G√∂n√ºll√ºl√ºk Algƒ±sƒ±_chunks.jsonl: 194 chunks\n",
      "   ‚úÖ 49) Yerel Yonetimlere Iliskin PolitikaBelgesi_chunks.jsonl: 176 chunks\n",
      "   ‚úÖ 5) Paralel Kariyer Arayƒ±≈üƒ±nƒ±n Nedenleri Isparta‚Äôda Faaliyet G√∂steren STK‚Äôlarda Bir Ara≈ütƒ±rma_chunks.jsonl: 249 chunks\n",
      "   ‚úÖ 50) Milliyet√ßiliƒüin D√∂n√º≈ü√ºm√º ve Gen√ß Y√ºzleri_chunks.jsonl: 2388 chunks\n",
      "   ‚úÖ 51)Gen√ßlik Ara≈ütƒ±rmasƒ± - T√ºrkiye 2024_chunks.jsonl: 564 chunks\n",
      "   ‚úÖ 52)CORE Gen√ßlerin Se√ßimi_chunks.jsonl: 222 chunks\n",
      "   ‚úÖ 53) Gen√ßlerin G√º√ßlendirilmesine Y√∂nelik Harcamalarƒ± ƒ∞zleme Kƒ±lavuzu_chunks.jsonl: 1229 chunks\n",
      "   ‚úÖ 54) Gen√ßlerin Siyasi Katƒ±lƒ±mƒ± - ≈ûebeke 1_chunks.jsonl: 3091 chunks\n",
      "   ‚úÖ 55) T√ºrkiye_de Gen√ßlerin Katƒ±lƒ±mƒ± - ≈ûebeke_chunks.jsonl: 1180 chunks\n",
      "   ‚úÖ 56) COVID-19 Pandemisi S√ºrecinde Gen√ßlerin ƒ∞yilik Halinin Belirlenmesi Ara≈ütƒ±rmasƒ±_chunks.jsonl: 1262 chunks\n",
      "   ‚úÖ 57) KONDA - Hafƒ±za Merkezi GencÃßlerin IÃánsan Haklarƒ± Algƒ±sƒ±__chunks.jsonl: 996 chunks\n",
      "   ‚úÖ 58) T√ºrkiye_de Gen√ßlik √áalƒ±≈ümasƒ± ve Politikasƒ±_chunks.jsonl: 5630 chunks\n",
      "   ‚úÖ 59) LGBTIÃá gencÃßler gencÃßlik merkezlerinde ne istiyor__chunks.jsonl: 216 chunks\n",
      "   ‚úÖ 6) Sivil Toplum √ñrg√ºtlerinde Profesyonel ve G√∂n√ºll√º √áalƒ±≈üma ƒ∞li≈ükileri Tehditler Ve Fƒ±rsatlar_chunks.jsonl: 244 chunks\n",
      "   ‚úÖ 60) GencÃß Kadƒ±nlarƒ±n Karar Alma Mekanizmalarƒ±na Katƒ±lƒ±mƒ±_chunks.jsonl: 154 chunks\n",
      "   ‚úÖ 61) Youth Policy Implementation at the Local Level- Imereti and Tbilisi_chunks.jsonl: 1140 chunks\n",
      "   ‚úÖ 62) TGSP TuÃàrkiye_nin GencÃßleri Yurtdƒ±sÃßƒ± Algƒ±sƒ±_chunks.jsonl: 62 chunks\n",
      "   ‚úÖ 63) TGSP TuÃàrkiye_nin GencÃßleri Dindarlƒ±k Algƒ±sƒ±_chunks.jsonl: 212 chunks\n",
      "   ‚úÖ 64) TGSP TuÃàrkiye_nin GencÃßleri YuÃàksekoÃàgÃÜrenim Algƒ±sƒ±_chunks.jsonl: 276 chunks\n",
      "   ‚úÖ 65 ) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± 2023_chunks.jsonl: 259 chunks\n",
      "   ‚úÖ 66) SODEV Genclik Arastirmasi Raporu 2021_chunks.jsonl: 29 chunks\n",
      "   ‚úÖ 67) SERHAT TRA2 GencÃßlik ArasÃßtƒ±rmasƒ±_chunks.jsonl: 5326 chunks\n",
      "   ‚úÖ 68) ƒ∞PA ƒ∞stanbulda Gencligin Demografik ve Sosyoekonomik Profili 20 yillik degisim_chunks.jsonl: 1351 chunks\n",
      "   ‚úÖ 69) ƒ∞PA √úniversite Mezunu Ev Gen√ßleri Ara≈ütƒ±rmasƒ±_chunks.jsonl: 56 chunks\n",
      "   ‚úÖ 7) e≈üitsiz demokrasiler_chunks.jsonl: 348 chunks\n",
      "   ‚úÖ 70) IPM TuÃàrkiye_de GencÃßlerin Yurtdƒ±sÃßƒ±nda YasÃßama IÃástegÃÜi_chunks.jsonl: 108 chunks\n",
      "   ‚úÖ 71) IÃáPM TuÃàrkiye_de Akƒ±llƒ± KentlesÃßme ve GencÃßlik Politikalarƒ±_chunks.jsonl: 113 chunks\n",
      "   ‚úÖ 72) TUÃàSES GencÃßlerin Cinsel SagÃÜlƒ±k ve UÃàreme Saƒülƒ±ƒüƒ± ArasÃßtƒ±rmasƒ±_chunks.jsonl: 1651 chunks\n",
      "   ‚úÖ 73) FES Youth Study Southeast Europe _chunks.jsonl: 1710 chunks\n",
      "   ‚úÖ 74) Biarada IÃástanbul‚Äôda GencÃßlik, Kent YurttasÃßlƒ±gÃÜƒ± ve Yerel YoÃànetim_chunks.jsonl: 1715 chunks\n",
      "   ‚úÖ 75) TuÃàrkiye GencÃßlik ArasÃßtƒ±rmasƒ± OÃàzet Bulgular 2023_chunks.jsonl: 186 chunks\n",
      "   ‚úÖ 76) Toplum CÃßalƒ±sÃßmasƒ± EnstituÃàsuÃà Kim Bu GencÃßler__chunks.jsonl: 119 chunks\n",
      "   ‚úÖ 77) KAOS GL LGBTIÃá+ OÃàgÃÜrenciler_chunks.jsonl: 1256 chunks\n",
      "   ‚úÖ 78) TOG UÃàniversiteli GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2024_chunks.jsonl: 230 chunks\n",
      "   ‚úÖ 79) Haberlerdeki √úniversite 2022_chunks.jsonl: 617 chunks\n",
      "   ‚úÖ 8) genc-oy-strateji-rapor_chunks.jsonl: 1281 chunks\n",
      "   ‚úÖ 80) TOG GencÃßlerin IÃáhtiyacÃßlarƒ± ArasÃßtƒ±rmasƒ± 2022_chunks.jsonl: 114 chunks\n",
      "   ‚úÖ 81) Yereliz GENCÃßLIÃáK ALANINDA CÃßALISÃßAN SIÃáVIÃáL TOPLUM OÃàRGUÃàTLERIÃá IÃáCÃßIÃáN YEREL SAVUNUCULUK REHBERIÃá_chunks.jsonl: 359 chunks\n",
      "   ‚úÖ 82) KONDA Barometre 2024_chunks.jsonl: 256 chunks\n",
      "   ‚úÖ 83) OECD Youth Policy Toolkit_chunks.jsonl: 3403 chunks\n",
      "   ‚úÖ 84) TIÃáP_li OÃàgÃÜrenciler Barƒ±nma Raporu 2023_chunks.jsonl: 710 chunks\n",
      "   ‚úÖ 85) ILO Global Employement Trends for Youth 2020_chunks.jsonl: 2269 chunks\n",
      "   ‚úÖ 86) GoFor Hangi Gen√ß__chunks.jsonl: 533 chunks\n",
      "   ‚úÖ 87) GoFor 2023 Universiteler icin Uzaktan Egitim ve KYK Yurtlarindan Ogrencilerin Cikarilmasina Iliskin Kararlar Hakkinda Bilgi Notu_chunks.jsonl: 57 chunks\n",
      "   ‚úÖ 88) TuÃàrkiye_de NEET UÃàzerine Yapƒ±lmƒ±sÃß √áalƒ±≈ümalara ƒ∞li≈ükin Bir DegÃÜerlendirme 2024_chunks.jsonl: 494 chunks\n",
      "   ‚úÖ 89) UNFPA IÃástatisliklerle GencÃßlik_chunks.jsonl: 1 chunks\n",
      "   ‚úÖ 9) Politik Karar Verme S√ºre√ßlerine Etkili ve Anlamlƒ± KATILIM HAKKI ve MEKANƒ∞ZMALAR_chunks.jsonl: 1648 chunks\n",
      "   ‚úÖ 90) FES Genclerin GoÃàzuÃànden Dindar-SekuÃàler Eksenli KutuplasÃßma_chunks.jsonl: 3318 chunks\n",
      "   ‚úÖ 91) Veriler_chunks.jsonl: 94 chunks\n",
      "\n",
      "‚úÖ Total chunks loaded: 90801\n",
      "üîç Checking for existing chunks in MongoDB...\n",
      "‚úÖ Found 0 existing chunks in database\n",
      "\n",
      "üìä Deduplication Status:\n",
      "   Total chunks in files: 90801\n",
      "   Already in database: 0\n",
      "   New chunks to process: 90801\n",
      "\n",
      "‚ö†Ô∏è  Warning: Found 5630 duplicate chunk_ids in source files\n",
      "   First few: [0, 1, 2, 3, 4]\n",
      "\n",
      "ü§ñ Generating embeddings for 90801 chunks...\n",
      "   Using model: text-embedding-3-large\n",
      "   Batch size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|‚ñè         | 22/909 [00:18<06:56,  2.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error generating embeddings for batch 22: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 9613 tokens (9613 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|‚ñé         | 23/909 [00:19<06:05,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error generating embeddings for batch 23: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10089 tokens (10089 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|‚ñé         | 24/909 [00:19<05:55,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error generating embeddings for batch 24: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10967 tokens (10967 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|‚ñé         | 25/909 [00:19<05:23,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error generating embeddings for batch 25: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13062 tokens (13062 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|‚ñé         | 27/909 [00:21<07:46,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error generating embeddings for batch 27: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 9896 tokens (9896 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|‚ñà‚ñé        | 121/909 [01:15<08:08,  1.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Process all JSONL files in the chunks folder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43membed_and_insert_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNKS_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Get statistics\u001b[39;00m\n\u001b[32m      6\u001b[39m     get_collection_stats()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36membed_and_insert_chunks\u001b[39m\u001b[34m(chunks_folder, force_reindex)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     embeddings = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚ùå Error generating embeddings for batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi//BATCH_SIZE\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(texts, client)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_embeddings\u001b[39m(texts: List[\u001b[38;5;28mstr\u001b[39m], client: OpenAI) -> List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    103\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate embeddings for a batch of texts using OpenAI.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEMBEDDING_MODEL\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [item.embedding \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\polcon\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSONL files in the chunks folder\n",
    "    embed_and_insert_chunks(chunks_folder=CHUNKS_FOLDER, force_reindex=False)\n",
    "    \n",
    "    # Get statistics\n",
    "    get_collection_stats()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ All done! Next steps:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"1. Create the vector search index in Atlas UI (see instructions above)\")\n",
    "    print(\"2. Wait 5-10 minutes for index to build\")\n",
    "    print(\"3. Test queries using query_vector_store() function\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Example query (uncomment after creating the vector index)\n",
    "    # print(\"\\n\" + \"=\"*70)\n",
    "    # print(\"Testing Query (after index is ready):\")\n",
    "    # print(\"=\"*70)\n",
    "    # results = query_vector_store(\"What is machine learning?\", top_k=3)\n",
    "    # for i, result in enumerate(results, 1):\n",
    "    #     print(f\"\\n{i}. Score: {result['score']:.4f}\")\n",
    "    #     print(f\"   Chunk ID: {result['chunk_id']}\")\n",
    "    #     print(f\"   Source: {result['source_file']}\")\n",
    "    #     print(f\"   Words: {result['word_count']}\")\n",
    "    #     print(f\"   Text preview: {result['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49d0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
